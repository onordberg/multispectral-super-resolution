{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and descriptive statistics of satellite imagery\n",
    "\n",
    "This jupyter notebook will parse, collate and analyze satellite imagery files from Maxar (former DigitalGlobe) The parsing of the metadata files (`DeliveryMetadata.xml`) are mainly general and should work for any Maxar product delivery. After parsing of the metadata files into pandas dataframes the code is mainly project specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from modules.metadata_reader import img_metadata_to_dict, add_names_to_metadata_dict, dict_to_df\n",
    "\n",
    "# Path to location where individual satellite images are located\n",
    "DATA_PATH = 'data/toulon-laspezia'\n",
    "DATA_PATH_IS_RELATIVE = True\n",
    "\n",
    "# Name of metadata .xml file\n",
    "METADATA_NAME = 'DeliveryMetadata.xml'\n",
    "\n",
    "# Names of areas covered by satellite imagery\n",
    "AREAS = ['La_Spezia', 'Toulon'] # Spelled like the directory names\n",
    "\n",
    "# Speficy what the xmlns url on top of metadata .xml file is\n",
    "# (should be second line)\n",
    "XMLNS = 'http://xsd.digitalglobe.com/xsd/dm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata parsing from xml to pandas dataframe\n",
    "\n",
    "Every satellite image delivery from Maxar contains a `DeliveryMetadata.xml` file with important specifications for both the multispectral and panchromatic images. The following functions finds all the `DeliveryMetadata.xml` files contained in all subdirectories of a directory and parses them into the *Pandas DataFrame* format which will be used for further descriptive statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metadata_pan, img_metadata_ms = img_metadata_to_dict(METADATA_NAME, \n",
    "                                                         DATA_PATH, XMLNS, \n",
    "                                                         path_is_relative = DATA_PATH_IS_RELATIVE)\n",
    "\n",
    "img_metadata_pan = add_names_to_metadata_dict(img_metadata_pan, AREAS)\n",
    "img_metadata_ms = add_names_to_metadata_dict(img_metadata_ms, AREAS)\n",
    "\n",
    "img_metadata_pan = dict_to_df(img_metadata_pan)\n",
    "img_metadata_ms = dict_to_df(img_metadata_ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some quick sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of panchromatic images:', len(img_metadata_pan))\n",
    "print('Number of multispectral images:', len(img_metadata_ms), '\\n')\n",
    "if list(img_metadata_pan.index.values) == list(img_metadata_ms.index.values):\n",
    "    print('Pass: Identical keys. Keys in panchromatic and multispectral dictionaries are identical.')\n",
    "else: print('Fail: Identical keys. Keys in panchromatic and multispectral dictionaries are not identical.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms multispectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_factors = ['pixelWidth', \n",
    "                'pixelHeight', \n",
    "                'n_bands',\n",
    "                'offNadirAngle', \n",
    "                'sunAzimuth', \n",
    "                'sunElevation']\n",
    "hist_ms = img_metadata_ms[hist_factors].hist(figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_pan = img_metadata_pan[hist_factors].hist(figsize=(15,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency table Sensor x Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(img_metadata_pan['area_name'], \n",
    "                    img_metadata_pan['sensorVehicle'], \n",
    "                    margins=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clouds?\n",
    "*TODO: Get a measure of clod coverage in every specific image. Either automatically based on available algorithms or manually since the number of images are limited.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate actual area covered by each image\n",
    "*TODO: Many images do not cover the whole area of interest. By using the parsed metadata dataframe and shape geometry supplied with every image it is possible to calculate the actual area. This number may nuance distribution of images across areas, sensors and clouds.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly select two images for early trials\n",
    "\n",
    "Before deciding on final train/validation/test split we randomly select two images from Toulon taken by the WorldView02 (WV02) sensor. These two images will in the future be inclued in the training partition. We do this in order to get started with early trials without compromising the validity of test results by experimenting on images that in the future will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toulon_wv02_pan = img_metadata_pan[(img_metadata_pan['sensorVehicle'] == 'WV02')\n",
    "                                   & (img_metadata_pan['area_name'] == 'Toulon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "img_names = sorted(toulon_wv02_pan.index.values)\n",
    "np.random.shuffle(img_names)\n",
    "images_for_early_trials = img_names[:2]\n",
    "images_for_early_trials"
   ]
  }
 ],
 "metadata": {
  "keep_output": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
