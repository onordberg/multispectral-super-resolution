{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 01: Band configurations\n",
    "\n",
    "In this experiment we will study how different band configurations affect the training process and end results.\n",
    "\n",
    "### Experiment variations:\n",
    "\n",
    "- E01-8: All 8 MS WorldView-2 bands is used.\n",
    "- E01-6: The 6 MS WorldView-2 bands that overlap the PAN band (RGB+Y+RedEdge+NIR1)\n",
    "- E01-4: The 4 MS WorldView-2 bands that are also available in GeoEye-1 (MS1 array RGB+NIR1)\n",
    " - In this variation we will also validate performance on the GeoEye-1 validation set\n",
    "- E01-3: Only the 3 RGB bands from WorldView-2\n",
    " - Also validated on the GeoEye-1 validation set\n",
    " \n",
    "### The notebook is divided into the following main sections:\n",
    "1. Imports and configuration parameters\n",
    "2. Tile generation (sampling of tiles from the satellite images)\n",
    "3. Building of models and tile input pipelines (`tf.dataset` objects reading tiles from disk)\n",
    "4. Pretraining with L1 loss\n",
    "5. GAN-training with L1 + Percep + GAN loss\n",
    "6. Inspection of results\n",
    "\n",
    "Training history is logged with TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Internal modules\n",
    "from modules.helpers import *\n",
    "from modules.tile_generator import *\n",
    "from modules.matlab_metrics import *\n",
    "from modules.image_utils import *\n",
    "from modules.tile_input_pipeline import *\n",
    "from modules.models import *\n",
    "from modules.logging import *\n",
    "\n",
    "# Check GPUs and enable dynamic GPU memory use:\",\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Prevent TensorFlow from allocating all memory of all GPUs:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAMES = ['e01-8', 'e01-6', 'e01-4', 'e01-3']\n",
    "EXPERIMENT = EXPERIMENT_NAMES[2]\n",
    "GENERATE_TILES = False\n",
    "\n",
    "# Load metadata dataframe from repository root\n",
    "meta = load_meta_pickle_csv('.', 'metadata_df', from_pickle=True)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = 'data/toulon-laspezia'\n",
    "DATA_PATH_TILES = 'data/toulon-laspezia-tiles/e01'\n",
    "DATA_PATH_TILES_P = {'train': DATA_PATH_TILES + '/train', \n",
    "                     'val': DATA_PATH_TILES + '/val', \n",
    "                     'test': DATA_PATH_TILES + '/test'}\n",
    "\n",
    "LOGS_DIR = 'logs/'\n",
    "LOGS_EXP_DIR = LOGS_DIR + EXPERIMENT\n",
    "\n",
    "# Subset by sensor and area\n",
    "SENSORS_GENERATE = ['WV02', 'GE01']\n",
    "AREAS_GENERATE = ['La_Spezia', 'Toulon']\n",
    "meta = subset_by_areas_sensor(meta, areas=AREAS_GENERATE, sensors=SENSORS_GENERATE)\n",
    "print('Sensors to generate tiles from:', SENSORS_GENERATE)\n",
    "print('Areas to generate tiles from:', AREAS_GENERATE)\n",
    "\n",
    "# Count images in partitions\n",
    "N_IMAGES_TOTAL = count_images(meta)\n",
    "N_IMAGES = {'train': count_images_in_partition(meta, 'train'), \n",
    "            'val': count_images_in_partition(meta, 'val'), \n",
    "            'test': count_images_in_partition(meta, 'test')}\n",
    "assert N_IMAGES_TOTAL == sum(N_IMAGES.values())  # Verify that different ways of counting adds up\n",
    "print('Number of images in partitions', N_IMAGES)\n",
    "print('Total number of images:', N_IMAGES_TOTAL)\n",
    "\n",
    "# Sensors used in which experiment variation\n",
    "SENSORS_EXP = {'e01-8': {'train': 'WV02', 'val': ['WV02'], 'test': ['WV02']}, \n",
    "               'e01-6': {'train': 'WV02', 'val': ['WV02'], 'test': ['WV02']}, \n",
    "               'e01-4': {'train': 'WV02', 'val': ['WV02', 'GE01'], 'test': ['WV02', 'GE01']}, \n",
    "               'e01-3': {'train': 'WV02', 'val': ['WV02', 'GE01'], 'test': ['WV02', 'GE01']}}\n",
    "SENSORS = SENSORS_EXP[EXPERIMENT]\n",
    "\n",
    "# Areas used in which experiment variation\n",
    "AREAS_EXP = {'e01-8': {'train': AREAS_GENERATE, 'val': AREAS_GENERATE, 'test': AREAS_GENERATE}, \n",
    "             'e01-6': {'train': AREAS_GENERATE, 'val': AREAS_GENERATE, 'test': AREAS_GENERATE}, \n",
    "             'e01-4': {'train': AREAS_GENERATE, 'val': AREAS_GENERATE, 'test': AREAS_GENERATE}, \n",
    "             'e01-3': {'train': AREAS_GENERATE, 'val': AREAS_GENERATE, 'test': AREAS_GENERATE}}\n",
    "AREAS = AREAS_EXP[EXPERIMENT]\n",
    "\n",
    "\n",
    "# Set tile dimensions\n",
    "SR_FACTOR = 4\n",
    "MS_SIZE = {'train': 32, 'val': 128, 'test': 128}\n",
    "PAN_SIZE = {'train': MS_SIZE['train'] * SR_FACTOR, \n",
    "            'val': MS_SIZE['val'] * SR_FACTOR, \n",
    "            'test': MS_SIZE['test'] * SR_FACTOR}\n",
    "print('MS (LR) tile size:', MS_SIZE)\n",
    "print('PAN (HR) tile size:', PAN_SIZE)\n",
    "print('SR factor:', SR_FACTOR)\n",
    "\n",
    "# Band (channel) configurations:\n",
    "N_PAN_BANDS = 1\n",
    "\n",
    "WV02_FULL_BAND_CONFIG = get_sensor_bands('WV02', meta)\n",
    "WV02_EXP_BAND_CONFIGS = {'e01-8': WV02_FULL_BAND_CONFIG, \n",
    "                        'e01-6': {k:v for (k,v) in WV02_FULL_BAND_CONFIG.items() if k not in ['Coastal', 'NIR2']}, \n",
    "                        'e01-4': {k:v for (k,v) in WV02_FULL_BAND_CONFIG.items() if k in ['Blue', 'Green', 'Red', 'NIR']},\n",
    "                        'e01-3': {k:v for (k,v) in WV02_FULL_BAND_CONFIG.items() if k in ['Blue', 'Green', 'Red']}}\n",
    "MS_BANDS_WV02_CONFIG = WV02_EXP_BAND_CONFIGS[EXPERIMENT]\n",
    "if EXPERIMENT == 'e01-8':\n",
    "    MS_BANDS_WV02_IDXS = 'all'\n",
    "else:\n",
    "    MS_BANDS_WV02_IDXS = list(MS_BANDS_WV02_CONFIG.values())\n",
    "N_MS_BANDS = len(MS_BANDS_WV02_CONFIG.values())\n",
    "\n",
    "GE01_FULL_BAND_CONFIG = get_sensor_bands('GE01', meta)\n",
    "GE01_EXP_BAND_CONFIGS = {'e01-8': {None: None}, \n",
    "                        'e01-6': {None: None}, \n",
    "                        'e01-4': GE01_FULL_BAND_CONFIG, \n",
    "                        'e01-3': {k:v for (k,v) in GE01_FULL_BAND_CONFIG.items() if k not in ['NIR']}}\n",
    "MS_BANDS_GE01_CONFIG = GE01_EXP_BAND_CONFIGS[EXPERIMENT]\n",
    "if EXPERIMENT == 'e01-4':\n",
    "    MS_BANDS_GE01_IDXS = 'all'\n",
    "else:\n",
    "    MS_BANDS_GE01_IDXS = list(MS_BANDS_GE01_CONFIG.values())\n",
    "print('MS (LR) Band Config WV02:', MS_BANDS_WV02_CONFIG)\n",
    "print('MS (LR) Band Config GE01:', MS_BANDS_GE01_CONFIG)\n",
    "\n",
    "TILES_PER_M2 = {'train': 0.01, \n",
    "                'val': 0.01, \n",
    "                'test': 0.01}\n",
    "\n",
    "RESIZE_TO_PIXEL_SIZE = False\n",
    "RESIZE_RESAMPLING_METHOD = 'nearest'  # 'nearest', 'bicubic', 'bilinear'\n",
    "\n",
    "CLOUD_SEA_REMOVAL = True\n",
    "CLOUD_SEA_WEIGHTS_PATH = 'models/cloud-sea-classifier/cloudsea-effb0-augm-bicubic-pan-0.0005--200-0.129130.h5'\n",
    "CLOUD_SEA_PRED_CUTOFF = 0.95\n",
    "\n",
    "BATCH_SIZE = {'train': 16, 'val': 8, 'test': 8}\n",
    "\n",
    "SHAVE_WIDTH = 4\n",
    "MATLAB_PATH = 'modules/matlab'\n",
    "\n",
    "\n",
    "# OPTIMIZeER parameters\n",
    "PRETRAIN_LOSS = 'l1'\n",
    "#PRETRAIN_LR = 0.0002 # Official\n",
    "PRETRAIN_LR = 5e-5\n",
    "PRETRAIN_BETA_1 = 0.9\n",
    "PRETRAIN_BETA_2 = 0.999\n",
    "\n",
    "#GAN_G_LR = 1e-4 # Official\n",
    "#GAN_D_LR = 1e-4 # Official\n",
    "#GAN_G_LR = 5e-5\n",
    "#GAN_D_LR = 5e-5\n",
    "GAN_G_LR = 2e-5\n",
    "GAN_D_LR = 2e-5\n",
    "G_BETA_1, D_BETA_1 = 0.9, 0.9\n",
    "G_BETA_2, D_BETA_2 = 0.999, 0.999\n",
    "\n",
    "G_LOSS_PIXEL_W = 0.01\n",
    "G_LOSS_PERCEP_W = 1.0\n",
    "G_LOSS_GENERATOR_W = 0.005\n",
    "\n",
    "# RRDB Model parameters \n",
    "N_BLOCKS = 16\n",
    "#N_BLOCKS = 23\n",
    "N_FILTERS = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tile generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Image resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tile allocation\n",
    "\n",
    "We allocate `n_tiles` to each satellite image in proportion to the area covered by the satellite image. We adjust `n_tiles` by the argument `tiles_per_m2`. If `tiles_per_m2=1.0` then `n_tiles` is set deterministically to a value so that a square meter of satellite image is expected to be covered by `1.0` tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = allocate_tiles_by_expected(meta, \n",
    "                                  override_pan_pixel_size=RESIZE_TO_PIXEL_SIZE,\n",
    "                                  by_partition=True, \n",
    "                                  tiles_per_m2_train_val_test=(TILES_PER_M2['train'], \n",
    "                                                               TILES_PER_M2['val'], \n",
    "                                                               TILES_PER_M2['test']),\n",
    "                                  pan_tile_size_train_val_test=(PAN_SIZE['train'], \n",
    "                                                                PAN_SIZE['val'], \n",
    "                                                                PAN_SIZE['test']),\n",
    "                                  new_column_name='n_tiles')\n",
    "\n",
    "n_tiles = {'train': count_tiles_in_partition(meta, 'train'),\n",
    "           'val': count_tiles_in_partition(meta, 'val'), \n",
    "           'test':  count_tiles_in_partition(meta, 'test')}\n",
    "n_tiles_total = count_tiles(meta)\n",
    "assert n_tiles_total == sum(n_tiles.values())\n",
    "print('Number of tiles per partition:')\n",
    "print(n_tiles)\n",
    "print('Total number of tiles:', n_tiles_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tile generation to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TILES:\n",
    "    meta = generate_all_tiles(meta, \n",
    "                              save_dir=DATA_PATH_TILES, \n",
    "                              sr_factor=SR_FACTOR, \n",
    "                              by_partition=True,\n",
    "                              ms_tile_size_train_val_test=(MS_SIZE['train'], MS_SIZE['val'], MS_SIZE['test']), \n",
    "                              cloud_sea_removal=CLOUD_SEA_REMOVAL, \n",
    "                              cloud_sea_weights_path=CLOUD_SEA_WEIGHTS_PATH, \n",
    "                              cloud_sea_pred_cutoff=CLOUD_SEA_PRED_CUTOFF,\n",
    "                              save_meta_to_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tiles_mean, train_tiles_sd = mean_sd_of_train_tiles(DATA_PATH_TILES, \n",
    "                                                          sample_proportion=1.0, \n",
    "                                                          write_json=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data input pipeline from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = {'train': n_tiles['train'],  # 100\n",
    "                       'val': n_tiles['val'],  # 100\n",
    "                       'test': n_tiles['test']}  # 100\n",
    "\n",
    "train_tiles_mean, train_tiles_sd = read_mean_sd_json(DATA_PATH_TILES)\n",
    "\n",
    "train_val_test = 'train'\n",
    "sensor = SENSORS[train_val_test]\n",
    "ds_train = GeotiffDataset(tiles_path=DATA_PATH_TILES_P[train_val_test], \n",
    "                          batch_size=BATCH_SIZE[train_val_test], \n",
    "                          ms_tile_shape=(MS_SIZE[train_val_test], MS_SIZE[train_val_test], N_MS_BANDS), \n",
    "                          pan_tile_shape=(PAN_SIZE[train_val_test], PAN_SIZE[train_val_test], N_PAN_BANDS),\n",
    "                          sensor=sensor,\n",
    "                          band_selection=MS_BANDS_WV02_IDXS, \n",
    "                          mean_correction=train_tiles_mean,\n",
    "                          cache_memory=True,\n",
    "                          cache_file=str(DATA_PATH_TILES + '/ds_' + EXPERIMENT + '-'\n",
    "                                           + train_val_test + '-' + sensor + '_cache'), \n",
    "                          repeat=True, \n",
    "                          shuffle=True, \n",
    "                          shuffle_buffer_size=SHUFFLE_BUFFER_SIZE[train_val_test])\n",
    "# Getting the scaled output range from the scaler. Needed to calculate PSNR and SSIM:\n",
    "scaled_range = ds_train.get_scaler_output_range(print_ranges=True)\n",
    "\n",
    "# Returning the actual tf.data.dataset object:\n",
    "ds_train = ds_train.get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set can have several sensors and is organized in a dictionary\n",
    "# structure: ds_val = {sensor: dataset} ... ex: ds_val = {'WV02': dataset_with_only_WV02_images}\n",
    "train_val_test = 'val'\n",
    "ds_val = {}\n",
    "for sensor in SENSORS[train_val_test]:\n",
    "    if sensor == 'WV02':\n",
    "        band_indices = MS_BANDS_WV02_IDXS\n",
    "    elif sensor == 'GE01':\n",
    "        band_indices = MS_BANDS_GE01_IDXS\n",
    "    ds_val[sensor] = GeotiffDataset(tiles_path=DATA_PATH_TILES_P[train_val_test], \n",
    "                                    batch_size=BATCH_SIZE[train_val_test], \n",
    "                                    ms_tile_shape=(MS_SIZE[train_val_test], MS_SIZE[train_val_test], N_MS_BANDS), \n",
    "                                    pan_tile_shape=(PAN_SIZE[train_val_test], PAN_SIZE[train_val_test], N_PAN_BANDS),\n",
    "                                    sensor=sensor,\n",
    "                                    band_selection=band_indices, \n",
    "                                    mean_correction=train_tiles_mean,\n",
    "                                    cache_memory=True,\n",
    "                                    cache_file=str(DATA_PATH_TILES + '/ds_' + EXPERIMENT + '-'\n",
    "                                                   + train_val_test + '-' + sensor + '_cache'), \n",
    "                                    repeat=True, \n",
    "                                    shuffle=True, \n",
    "                                    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE[train_val_test])\n",
    "    ds_val[sensor] = ds_val[sensor].get_dataset()\n",
    "print(ds_val.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test = 'test'\n",
    "ds_test = {}\n",
    "for sensor in SENSORS[train_val_test]:\n",
    "    if sensor == 'WV02':\n",
    "        band_indices = MS_BANDS_WV02_IDXS\n",
    "    elif sensor == 'GE01':\n",
    "        band_indices = MS_BANDS_GE01_IDXS\n",
    "    ds_test[sensor] = GeotiffDataset(tiles_path=DATA_PATH_TILES_P[train_val_test], \n",
    "                                     batch_size=BATCH_SIZE[train_val_test], \n",
    "                                     ms_tile_shape=(MS_SIZE[train_val_test], MS_SIZE[train_val_test], N_MS_BANDS), \n",
    "                                     pan_tile_shape=(PAN_SIZE[train_val_test], PAN_SIZE[train_val_test], N_PAN_BANDS),\n",
    "                                     sensor=sensor,\n",
    "                                     band_selection=band_indices, \n",
    "                                     mean_correction=train_tiles_mean,\n",
    "                                     cache_memory=False,\n",
    "                                     cache_file=str(DATA_PATH_TILES + '/ds_' + EXPERIMENT + '-'\n",
    "                                                    + train_val_test + '-' + sensor + '_cache'), \n",
    "                                     repeat=False, \n",
    "                                     shuffle=False, \n",
    "                                     shuffle_buffer_size=SHUFFLE_BUFFER_SIZE[train_val_test])\n",
    "    ds_test[sensor] = ds_test[sensor].get_dataset()\n",
    "print(ds_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(ds_val['GE01']):\n",
    "    print(batch[0].shape)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Bicubic baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicubic = build_deterministic_sr_model(upsample_factor=SR_FACTOR,\n",
    "                                       resize_method='bicubic',\n",
    "                                       loss='mean_absolute_error',\n",
    "                                       metrics=('PSNR', 'SSIM'),\n",
    "                                       scaled_range=scaled_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ESRGAN Generator model (pretrain version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model =  build_generator(pretrain_or_gan='pretrain', \n",
    "                                  pretrain_learning_rate=PRETRAIN_LR, \n",
    "                                  pretrain_loss_l1_l2=PRETRAIN_LOSS,\n",
    "                                  pretrain_beta_1=PRETRAIN_BETA_1, \n",
    "                                  pretrain_beta_2=PRETRAIN_BETA_2, \n",
    "                                  pretrain_metrics=('PSNR', 'SSIM'),\n",
    "                                  scaled_range=scaled_range, \n",
    "                                  n_channels_in=N_MS_BANDS, \n",
    "                                  n_channels_out=N_PAN_BANDS, \n",
    "                                  height_width_in=None,  # None will make network image size agnostic\n",
    "                                  n_filters=N_FILTERS, \n",
    "                                  n_blocks=N_BLOCKS)\n",
    "# pretrain_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = EsrganLogger(\n",
    "    model_name=EXPERIMENT + '-pretrain',\n",
    "    log_tensorboard=True,\n",
    "    tensorboard_logs_dir=LOGS_EXP_DIR + '/tb',\n",
    "    save_models=True,\n",
    "    models_save_dir=LOGS_EXP_DIR + '/models',\n",
    "    save_weights_only=True,\n",
    "    log_train_images=False,\n",
    "    model=pretrain_model,\n",
    "    n_train_image_batches=1,\n",
    "    train_image_dataset=ds_train,\n",
    "    log_val_images=False,\n",
    "    n_val_image_batches=1,\n",
    "    val_image_dataset=None,  # This can also be dict with different sensors\n",
    "    log_val_secondary_sensor=True,\n",
    "    val_second_dataset=ds_test['GE01'],\n",
    "    val_second_name='GE01',\n",
    "    val_second_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.get_callbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_name, val_ds in ds_val.items():\n",
    "    print(val_name, val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_batch(iter(ds_val['GE01']).get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "history = pretrain_model.fit(ds_train, \n",
    "                             epochs=EPOCHS, \n",
    "                             validation_data=ds_val['WV02'],\n",
    "                             steps_per_epoch=10, \n",
    "                             validation_steps=10, \n",
    "                             initial_epoch=0,\n",
    "                             callbacks=logger.get_callbacks()\n",
    "                             )\n",
    "#pretrain_model.save_weights(\"models/esrgan-psnr-train-WV02-val-WV02-98-0.001750.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
