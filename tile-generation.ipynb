{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satellite imagery tile generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import rasterio\n",
    "import rasterio.plot\n",
    "import geopandas\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "from modules.helpers import get_int_uid, get_string_uid\n",
    "\n",
    "# Check GPUs:\",\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Prevent TensorFlow from allocating all memory of all GPUs:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metadata_df.pickle', 'rb') as file:\n",
    "    meta = pickle.load(file)\n",
    "    \n",
    "# Path to location where individual satellite images are located\n",
    "DATA_PATH = 'data/toulon-laspezia' \n",
    "DATA_PATH_IS_RELATIVE = True\n",
    "DATA_PATH_TILES = 'data/toulon-laspezia-tiles/GE01'\n",
    "DATA_PATH_TILES_TRAIN = str(DATA_PATH_TILES + '/train')\n",
    "DATA_PATH_TILES_VAL = str(DATA_PATH_TILES + '/val')\n",
    "DATA_PATH_TILES_TEST = str(DATA_PATH_TILES + '/test')\n",
    "\n",
    "#SENSORS = ['WV02']\n",
    "#SENSORS = ['WV03_VNIR']\n",
    "SENSORS = ['GE01']\n",
    "\n",
    "AREAS = ['La_Spezia', 'Toulon']\n",
    "meta = meta.loc[meta['sensorVehicle'].isin(SENSORS)]\n",
    "meta = meta.loc[meta['area_name'].isin(AREAS)]\n",
    "\n",
    "N_IMAGES = len(meta.index)\n",
    "\n",
    "def count_images_in_partition(meta, train_val_test):\n",
    "    try: \n",
    "        n_images = meta['train_val_test'].value_counts()[train_val_test]\n",
    "    except KeyError as e:\n",
    "        n_images = 0\n",
    "    return n_images\n",
    "\n",
    "N_IMAGES_TRAIN = count_images_in_partition(meta, 'train')\n",
    "N_IMAGES_VAL = count_images_in_partition(meta, 'val')\n",
    "N_IMAGES_TEST = count_images_in_partition(meta, 'test')\n",
    "print(N_IMAGES_TRAIN, N_IMAGES_VAL, N_IMAGES_TEST)\n",
    "\n",
    "#N_TILES_TRAIN = 10000\n",
    "N_TILES_TRAIN = 0\n",
    "N_TILES_VAL = 2000\n",
    "N_TILES_TEST = 2000\n",
    "\n",
    "PAN_WIDTH, PAN_HEIGHT = (384, 384)\n",
    "PAN_PIXEL_WIDTH, PAN_PIXEL_HEIGHT = (0.5, 0.5)\n",
    "\n",
    "SR_FACTOR = 4\n",
    "MS_WIDTH, MS_HEIGHT = (int(PAN_WIDTH/SR_FACTOR), int(PAN_HEIGHT/SR_FACTOR))\n",
    "MS_PIXEL_WIDTH, MS_PIXEL_HEIGHT = (2.0, 2.0)\n",
    "\n",
    "RESAMPLE_TO_PIXEL_SIZE = True\n",
    "\n",
    "# Should be derived automatically, but added here as a quick fix\n",
    "MS_BANDS = 8\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile allocation\n",
    "\n",
    "The number of tiles from each satellite image is decided through a weighted sampling where the satellite image sizes are the weights. (More correctly the ratio between the size of the image of interest and the largest image are the weights). Some images only cover parts of the area of interest and this weighted sampling ensures that these smaller areas are not disproportionally represented during training, validation and/or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocating_tiles(meta, n_tiles_train, n_tiles_val, n_tiles_test):\n",
    "    counts_df = pd.DataFrame(index=meta.index)\n",
    "    counts_df['n_tiles'] = None\n",
    "\n",
    "    for p in ['train', 'val', 'test']:\n",
    "        if p == 'train' and n_tiles_train > 0:\n",
    "            n_tiles = n_tiles_train\n",
    "        elif p == 'val' and n_tiles_val > 0:\n",
    "            n_tiles = n_tiles_val\n",
    "        elif p == 'test' and n_tiles_test > 0:\n",
    "            n_tiles = n_tiles_test\n",
    "        else:\n",
    "            continue # If n_tiles_part is 0\n",
    "            \n",
    "        l = list(meta[meta['train_val_test'] == p]['area_ratio'].index)\n",
    "        w = list(meta[meta['train_val_test'] == p]['area_ratio'].values)\n",
    "\n",
    "        sampling = random.choices(l, weights = w, k = n_tiles)\n",
    "        print('Allocated', n_tiles, 'tiles across the', \n",
    "              meta['train_val_test'].value_counts()[p], \n",
    "              'images in the', p, 'partition.' )\n",
    "        counts = pd.DataFrame.from_dict(dict(Counter(sampling)), \n",
    "                                        orient = 'index', \n",
    "                                        columns = ['n_tiles'])\n",
    "        counts_df.update(counts)        \n",
    "    \n",
    "    meta['n_tiles'] = counts_df\n",
    "    meta['n_tiles'] = meta['n_tiles'].astype('int32')\n",
    "    return meta\n",
    "    \n",
    "meta = allocating_tiles(meta, N_TILES_TRAIN, N_TILES_VAL, N_TILES_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.to_csv(str(DATA_PATH_TILES + '/metadata_tile_allocation.csv'))\n",
    "with open(str(DATA_PATH_TILES + '/metadata_tile_allocation.pickle'), 'wb') as file:\n",
    "    pickle.dump(meta, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile generation to disk\n",
    "\n",
    "Tiles are uniformly sampled from the satellite images and saved to disk as individual geotiffs, thus preserving the geospatial information. If the sampled tile includes border area pixels (pixels with value 0 in our case) the tile is discarded and a new tile is sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_box(img_shape, crop_size):\n",
    "    maxval_y, maxval_x = img_shape[1:]\n",
    "    maxval_y -= crop_size[0]\n",
    "    maxval_x -= crop_size[1]\n",
    "    rng = np.random.default_rng()\n",
    "    upper_left_yx = rng.integers(0, high=[maxval_y, maxval_x], dtype='int32')\n",
    "    return np.concatenate((upper_left_yx, np.array(crop_size), np.array([img_shape[0]])))\n",
    "\n",
    "def get_hr_box(lr_box, resize_factor, hr_channels):\n",
    "    hr_box = np.copy(lr_box)\n",
    "    hr_box[:4] = lr_box[:4] * resize_factor\n",
    "    hr_box[4] = hr_channels\n",
    "    return hr_box\n",
    "\n",
    "def generate_tiles(row):\n",
    "    image_string_UID = get_string_uid(meta, row['int_uid'])\n",
    "    print(row['train_val_test'], image_string_UID, '- Generating', row['n_tiles'], 'tiles')\n",
    "    partition_dir = pathlib.Path(DATA_PATH_TILES, row['train_val_test'])\n",
    "    partition_dir.mkdir(exist_ok = True)\n",
    "    image_dir = pathlib.Path(partition_dir, image_string_UID)\n",
    "    image_dir.mkdir(exist_ok = True)\n",
    "    ms_dir = pathlib.Path(image_dir, 'ms')\n",
    "    ms_dir.mkdir(exist_ok = True)\n",
    "    pan_dir = pathlib.Path(image_dir, 'pan')\n",
    "    pan_dir.mkdir(exist_ok = True)\n",
    "\n",
    "    with rasterio.open(row['ms_tif_path'], 'r') as ms_src, rasterio.open(row['pan_tif_path'], 'r') as pan_src:\n",
    "        img_ms = ms_src.read()\n",
    "        img_pan = pan_src.read()\n",
    "\n",
    "        for i in range(row['n_tiles']):\n",
    "            #print(i)\n",
    "            while True:\n",
    "                ms_box = get_random_box(img_ms.shape, [MS_HEIGHT, MS_WIDTH])\n",
    "                ms_win = rasterio.windows.Window(ms_box[1], ms_box[0], ms_box[3], ms_box[2])\n",
    "                ms_tile = ms_src.read(window=ms_win)\n",
    "                ms_win_transform = ms_src.window_transform(ms_win)\n",
    "                \n",
    "                pan_box = get_hr_box(ms_box, SR_FACTOR, img_pan.shape[0])\n",
    "\n",
    "                pan_win = rasterio.windows.Window(pan_box[1], pan_box[0], pan_box[3], pan_box[2])\n",
    "                pan_tile = pan_src.read(window=pan_win)\n",
    "                pan_win_transform = pan_src.window_transform(pan_win)\n",
    "                \n",
    "                if 0 in ms_tile:\n",
    "                    print('Border area detected in ms tile', i, \n",
    "                          'from image', image_string_UID)\n",
    "                    print('Resampling tile')\n",
    "                elif 0 in pan_tile:\n",
    "                    print('Border area detected in pan tile', i, \n",
    "                          'from image', image_string_UID)\n",
    "                    print('Resampling tile')\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "            with rasterio.open(\n",
    "                pathlib.Path(ms_dir, str(str(i).zfill(5) + '.tif')),\n",
    "                'w',\n",
    "                driver='GTiff',\n",
    "                width=ms_box[3],\n",
    "                height=ms_box[2],\n",
    "                count=ms_box[4],\n",
    "                dtype=ms_tile.dtype,\n",
    "                crs=ms_src.crs,\n",
    "                transform=ms_win_transform) as ms_dst:\n",
    "                    ms_dst.write(ms_tile)\n",
    "                \n",
    "            with rasterio.open(\n",
    "                pathlib.Path(pan_dir, str(str(i).zfill(5) + '.tif')),\n",
    "                'w',\n",
    "                driver='GTiff',\n",
    "                width=pan_box[3],\n",
    "                height=pan_box[2],\n",
    "                count=pan_box[4],\n",
    "                dtype=pan_tile.dtype,\n",
    "                crs=pan_src.crs,\n",
    "                transform=pan_win_transform) as pan_dst:\n",
    "                    pan_dst.write(pan_tile)\n",
    "\n",
    "def generate_all_tiles(meta):\n",
    "    print('Generating', N_TILES_TRAIN, 'training,', \n",
    "          N_TILES_VAL, 'validation and', N_TILES_TEST, 'test tiles:')\n",
    "    meta.apply(generate_tiles, axis = 1)\n",
    "    print('Tile generation finished')\n",
    "    \n",
    "generate_all_tiles(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tile generator from disk\n",
    "\n",
    "Using `tf.data` API to construct a `Dataset` generator reading and preprocessing tiles from disk.\n",
    "\n",
    "Best practices from https://www.tensorflow.org/guide/data, including multithreading, prefetching, shuffling, batching and caching.\n",
    "\n",
    "`rasterio` is used to read geotiffs. the `decode_geotiff()` function is run inside a `tf.py_function()` wrapper ensuring that this function is also run in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_geotiff(image_path):\n",
    "    image_path = pathlib.Path(image_path.numpy().decode())\n",
    "    with rasterio.open(image_path) as src:\n",
    "        img = src.read()\n",
    "    img = rasterio.plot.reshape_as_image(img) # from channels first to channels last\n",
    "    return img\n",
    "\n",
    "def preprocess_images(img, ms_or_pan):\n",
    "    if ms_or_pan == 'ms':\n",
    "        h, w = MS_HEIGHT, MS_WIDTH\n",
    "    elif ms_or_pan == 'pan':\n",
    "        h, w = PAN_HEIGHT, PAN_WIDTH\n",
    "        \n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.reshape(img, [h, w, -1]) # To avoid issue with extra dimension\n",
    "    return img\n",
    "\n",
    "def upsample_images(ms_img, pan_img):\n",
    "    ms_img = tf.ensure_shape(ms_img, [MS_HEIGHT, MS_WIDTH, MS_BANDS])\n",
    "    ms_img = tf.image.resize(ms_img, [PAN_HEIGHT, PAN_WIDTH])\n",
    "    return ms_img, pan_img\n",
    "\n",
    "def process_path(ms_tile_path):\n",
    "    img_string_UID = tf.strings.split(ms_tile_path, os.sep)[-3]\n",
    "    tile_UID = tf.strings.split(tf.strings.split(ms_tile_path, os.sep)[-1], '.')[0]\n",
    "    \n",
    "    ms_img = tf.py_function(decode_geotiff, [ms_tile_path], [tf.int16])\n",
    "    pan_tile_path = tf.strings.regex_replace(ms_tile_path, '\\\\\\\\ms\\\\\\\\', '\\\\\\\\pan\\\\\\\\')\n",
    "    pan_img = tf.py_function(decode_geotiff, [pan_tile_path], [tf.int16])\n",
    "    \n",
    "    ms_img = preprocess_images(ms_img, 'ms')\n",
    "    pan_img = preprocess_images(pan_img, 'pan')\n",
    "    \n",
    "    return ms_img, pan_img\n",
    "\n",
    "# https://www.tensorflow.org/tutorials/load_data/images\n",
    "def prepare_for_training(ds, batch_size, cache=True, shuffle_buffer_size=100):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    #ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_tif_tiles(tiles_path, batch_size, upsampling = False, \n",
    "                           cache = True, shuffle_buffer_size = 1000):\n",
    "    \n",
    "    ds = tf.data.Dataset.list_files(str(pathlib.Path(tiles_path)/'*/ms*.tif'))\n",
    "    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # upsampling through bicubic convolution before SR is required for SRCNN\n",
    "    if upsampling:\n",
    "        ds = ds.map(upsample_images, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds = prepare_for_training(ds, batch_size, cache, shuffle_buffer_size)\n",
    "    return ds\n",
    "\n",
    "ds_train = dataset_from_tif_tiles(DATA_PATH_TILES_TRAIN, BATCH_SIZE, upsampling = False)\n",
    "ds_val = dataset_from_tif_tiles(DATA_PATH_TILES_VAL, BATCH_SIZE, upsampling = False)\n",
    "ds_test = dataset_from_tif_tiles(DATA_PATH_TILES_TEST, BATCH_SIZE, upsampling = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch):\n",
    "    ms = image_batch[0].numpy()\n",
    "    pan = image_batch[1].numpy()\n",
    "    print('ms batch shape', ms.shape)\n",
    "    print('pan batch shape', pan.shape)\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for i in range(8):\n",
    "        i = i * 2\n",
    "        ax_ms = plt.subplot(4,4,i+1, label = 'ms')\n",
    "        \n",
    "        ms_image = ms[i,:,:,2] # Just showing channel 2 as grayscale\n",
    "        pan_image = pan[i,:,:,0]\n",
    "\n",
    "        #plt.imshow(ms_image)\n",
    "        ax_ms.imshow(ms_image, cmap = 'gray')\n",
    "        \n",
    "        ax_pan = plt.subplot(4,4,i+2, label = 'pan')\n",
    "\n",
    "        ax_pan.imshow(pan_image, cmap = 'gray')\n",
    "        \n",
    "show_batch(next(iter(ds_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "keep_output": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
