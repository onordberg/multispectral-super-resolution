{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "from rasterio.plot import reshape_as_image\n",
    "\n",
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "from modules.metadata_reader import img_metadata_to_dict, add_names_to_metadata_dict, dict_to_df\n",
    "\n",
    "# Path to location where individual satellite images are located\n",
    "DATA_PATH = 'data/toulon-laspezia' \n",
    "DATA_PATH_IS_RELATIVE = True\n",
    "DATA_PATH_NPY = 'data/toulon-laspezia-npy' \n",
    "DATA_PATH_TILES = 'data/toulon-laspezia-tiles'\n",
    "\n",
    "# Name of metadata .xml file\n",
    "METADATA_NAME = 'DeliveryMetadata.xml'\n",
    "\n",
    "# Names of areas covered by satellite imagery\n",
    "AREAS = ['La_Spezia', 'Toulon'] # Spelled like the directory names\n",
    "\n",
    "# Speficy what the xmlns url on top of metadata .xml file is\n",
    "# (should be second line)\n",
    "XMLNS = 'http://xsd.digitalglobe.com/xsd/dm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata parsing from xml to pandas dataframe\n",
    "\n",
    "Every satellite image delivery from Maxar contains a `DeliveryMetadata.xml` file with important specifications for both the multispectral and panchromatic images. The following functions finds all the `DeliveryMetadata.xml` files contained in all subdirectories of a directory and parses them into the *Pandas DataFrame* format which will be used for further descriptive statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metadata_pan, img_metadata_ms = img_metadata_to_dict(METADATA_NAME, \n",
    "                                                         DATA_PATH, XMLNS, \n",
    "                                                         path_is_relative = DATA_PATH_IS_RELATIVE)\n",
    "\n",
    "img_metadata_pan = add_names_to_metadata_dict(img_metadata_pan, AREAS)\n",
    "img_metadata_ms = add_names_to_metadata_dict(img_metadata_ms, AREAS)\n",
    "\n",
    "img_metadata_pan = dict_to_df(img_metadata_pan)\n",
    "img_metadata_ms = dict_to_df(img_metadata_ms)\n",
    "\n",
    "# Checking that string IDs and int IDs are equal in both dataframes (they should)\n",
    "assert all(img_metadata_ms.index == img_metadata_pan.index)\n",
    "assert all(img_metadata_ms['int_uid'] == img_metadata_pan['int_uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metadata_pan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup functions for UIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_uid(string_UIDs):\n",
    "    return img_metadata_pan.loc[string_UIDs]['int_uid'].tolist()\n",
    "\n",
    "def get_string_uid(int_UIDs):\n",
    "    # Could probably be neater\n",
    "    # Accepts list of ints and single int\n",
    "    if type(int_UIDs) == int:\n",
    "        int_UIDs = [int_UIDs]\n",
    "    l = []\n",
    "    for int_UID in int_UIDs:\n",
    "        l.append(img_metadata_pan[img_metadata_pan['int_uid'] == int_UID].index.tolist()[0])\n",
    "    if len(l) == 1:\n",
    "        return l[0]\n",
    "    else:\n",
    "        return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly draw 2 images for early trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toulon_wv02_pan = img_metadata_pan[(img_metadata_pan['sensorVehicle'] == 'WV02')\n",
    "                                   & (img_metadata_pan['area_name'] == 'Toulon')]\n",
    "\n",
    "np.random.seed(1)\n",
    "img_names = sorted(toulon_wv02_pan.index.values)\n",
    "np.random.shuffle(img_names)\n",
    "images_for_early_trials = img_names[:2]\n",
    "images_for_early_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_for_early_trials_int_UIDs = get_int_uid(images_for_early_trials)\n",
    "images_for_early_trials_int_UIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = len(img_metadata_pan.index)\n",
    "\n",
    "PAN_WIDTH, PAN_HEIGHT = (384, 384)\n",
    "PAN_BANDS = 1\n",
    "\n",
    "SR_FACTOR = 4\n",
    "MS_WIDTH, MS_HEIGHT = (int(PAN_WIDTH/SR_FACTOR), int(PAN_HEIGHT/SR_FACTOR))\n",
    "MS_BANDS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert .tif files to .npy for easier loading later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tif_to_npy(path_in, filename, save_to_disk = False, path_out = None):\n",
    "    path_in = pathlib.Path(path_in)\n",
    "    #filename = path_in.stem\n",
    "    with rasterio.open(path_in, 'r') as ds:\n",
    "        img = ds.read()\n",
    "    print(type(img))\n",
    "    img = reshape_as_image(img)\n",
    "    print(img.shape)\n",
    "    if save_to_disk:\n",
    "        np.save(pathlib.Path(path_out, filename), img)\n",
    "        return True\n",
    "    return img\n",
    "\n",
    "def all_tif_to_npy(metadata_pan, metadata_ms, path_out):\n",
    "    \n",
    "    # Saving panchromatic images as .npy files\n",
    "    filenames = metadata_pan.index.values.tolist()\n",
    "    path_out_pan = pathlib.Path(path_out, 'pan')\n",
    "    for filename in filenames:\n",
    "        tif_to_npy(metadata_pan.loc[filename]['tif_path'], \n",
    "                   filename, save_to_disk = True,\n",
    "                   path_out = path_out_pan)\n",
    "        print('Saved', filename, 'in dir', str(path_out_pan))\n",
    "    \n",
    "    # Saving multispectral images as .npy files\n",
    "    filenames = metadata_ms.index.values.tolist()\n",
    "    path_out_ms = pathlib.Path(path_out, 'ms')\n",
    "    for filename in filenames:\n",
    "        tif_to_npy(metadata_ms.loc[filename]['tif_path'], \n",
    "                   filename, save_to_disk = True,\n",
    "                   path_out = path_out_ms)\n",
    "        print('Saved', filename, 'in', str(path_out_ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to actually convert (takes some time):\n",
    "\n",
    "#all_tif_to_npy(img_metadata_pan, img_metadata_ms, DATA_PATH_NPY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding paths to .npy files as column in metadata dataframe\n",
    "\n",
    "The metadata dataframes are kept up to date so that it can be used as a canonical source of information about images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_npy_paths_to_metadata_df(metadata_pan, metadata_ms, path_to_npy):\n",
    "    path_pan = pathlib.Path(pathlib.Path.cwd(), path_to_npy, 'pan')\n",
    "    pan_paths = list(path_pan.glob('**/*.npy'))\n",
    "    pan_names = [path.stem for path in pan_paths]\n",
    "    pan_path_df = pd.DataFrame({'pan_names':pan_names,'npy_path':pan_paths}).set_index('pan_names')\n",
    "    metadata_pan = pd.concat([metadata_pan, pan_path_df],axis=1)\n",
    "    \n",
    "    path_ms = pathlib.Path(pathlib.Path.cwd(), path_to_npy, 'ms')\n",
    "    ms_paths = list(path_ms.glob('**/*.npy'))\n",
    "    ms_names = [path.stem for path in ms_paths]\n",
    "    ms_path_df = pd.DataFrame({'ms_names':ms_names,'npy_path':ms_paths}).set_index('ms_names')\n",
    "    metadata_ms = pd.concat([metadata_ms, ms_path_df],axis=1)\n",
    "    \n",
    "    return metadata_pan, metadata_ms\n",
    "\n",
    "img_metadata_pan, img_metadata_ms = add_npy_paths_to_metadata_df(img_metadata_pan, \n",
    "                                                                 img_metadata_ms, \n",
    "                                                                 DATA_PATH_NPY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading .npy files into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy_to_dict(metadata_df, int_UID_list = None):\n",
    "    if int_UID_list == None:\n",
    "        int_UID_list = list(metadata_df['int_uid'].tolist())\n",
    "    d = {}\n",
    "    print('Loading .npy files')\n",
    "    for int_UID in int_UID_list:\n",
    "        d[str(int_UID)] = np.load(metadata_df.loc[get_string_uid(int_UID)]['npy_path'])\n",
    "        print(str(int_UID), ' - ', get_string_uid(int_UID), \n",
    "              'loaded into memory as ndarray with shape', d[str(int_UID)].shape)\n",
    "    return d\n",
    "\n",
    "#def load_npy_to_list(metadata_df, int_UID_list = None):\n",
    "#    if ID_list == None:\n",
    "#        ID_list = list(metadata_df.index.tolist())\n",
    "#    l = []\n",
    "#    for ID in ID_list:\n",
    "#        l.append(np.load(metadata_df.loc[ID]['npy_path']))\n",
    "#        print(ID, 'loaded into memory as ndarray with shape', l[-1].shape)\n",
    "#    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load only early trials images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_pan = load_npy_to_dict(img_metadata_pan, images_for_early_trials_int_UIDs)\n",
    "imgs_ms = load_npy_to_dict(img_metadata_ms, images_for_early_trials_int_UIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all images\n",
    "Keep a watch on available RAM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs_pan = load_npy_to_dict(img_metadata_pan)\n",
    "#imgs_ms = load_npy_to_dict(img_metadata_ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img, yxhwc_box):\n",
    "    img = tf.image.crop_to_bounding_box(\n",
    "        img, \n",
    "        offset_height = yxhwc_box[0], \n",
    "        offset_width = yxhwc_box[1], \n",
    "        target_height = yxhwc_box[2], \n",
    "        target_width = yxhwc_box[3])\n",
    "    return img\n",
    "\n",
    "def get_random_box(img_shape, crop_size):\n",
    "    maxval_y, maxval_x = img_shape[:2]\n",
    "    maxval_y -= crop_size[0]\n",
    "    maxval_x -= crop_size[1]\n",
    "    #print(maxval_y, maxval_x)\n",
    "    rng = np.random.default_rng()\n",
    "    upper_left_yx = rng.integers(0, high=[maxval_y, maxval_x], dtype='int32')\n",
    "    \n",
    "    #returning in yxhwc format\n",
    "    return np.concatenate((upper_left_yx, np.array(crop_size)))\n",
    "\n",
    "def get_hr_box(lr_box, resize_factor, channels):\n",
    "    hr_box = lr_box\n",
    "    hr_box[:4] = lr_box[:4] * resize_factor\n",
    "    hr_box[4] = channels\n",
    "    return hr_box\n",
    "\n",
    "def scale_image(img):\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def add_imgID(img_array, imgID):\n",
    "    img_array = np.expand_dims(img_array, 0)\n",
    "    img_array[:,]\n",
    "\n",
    "def preprocess_tiles(imgs_pan, imgs_ms, img_IDs, n_tiles):\n",
    "    n_images = len(img_IDs)\n",
    "    arr_ms = np.zeros((n_tiles, MS_HEIGHT, MS_WIDTH, MS_BANDS))\n",
    "    arr_pan = np.zeros((n_tiles, PAN_HEIGHT, PAN_WIDTH, PAN_BANDS))\n",
    "    print(arr_ms.shape)\n",
    "    tile_imgID_map = []\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    for i in range(n_tiles):\n",
    "        img_ID_int = rng.integers(0, high=n_images, dtype='int32')\n",
    "        img_ID = img_IDs[img_ID_int]\n",
    "        tile_imgID_map.append(img_ID)\n",
    "        print(img_ID)\n",
    "\n",
    "        img_pan = scale_image(imgs_pan[img_ID_int])\n",
    "        img_ms = scale_image(imgs_ms[img_ID_int])\n",
    "        \n",
    "        box_ms = get_random_box(img_ms.shape, [MS_HEIGHT, MS_WIDTH, MS_BANDS])\n",
    "        print(box_ms)\n",
    "        img_ms_cropped = crop(img_ms, box_ms)\n",
    "        box_pan = get_hr_box(box_ms, SR_FACTOR, PAN_BANDS)\n",
    "        img_pan_cropped = crop(img_pan, box_pan)\n",
    "        arr_ms[i,:,:,:] = img_ms_cropped\n",
    "        arr_pan[i,:,:,:] = img_pan_cropped\n",
    "    \n",
    "    return arr_ms, arr_pan, tile_imgID_map\n",
    "\n",
    "def generate_tiles(n_tiles, imgs_pan, imgs_ms, int_UIDs):\n",
    "    n_images = len(int_UIDs)\n",
    "    \n",
    "    for i in range(n_tiles):\n",
    "        # Draw which image to produce tile from\n",
    "        int_UID = random.choice(int_UIDs)\n",
    "        \n",
    "        img_pan = scale_image(imgs_pan[str(int_UID)])\n",
    "        img_ms = scale_image(imgs_ms[str(int_UID)])\n",
    "        \n",
    "        box_ms = get_random_box(img_ms.shape, [MS_HEIGHT, MS_WIDTH, MS_BANDS])\n",
    "        img_ms_cropped = crop(img_ms, box_ms)\n",
    "        \n",
    "        box_pan = get_hr_box(box_ms, SR_FACTOR, PAN_BANDS)\n",
    "        img_pan_cropped = crop(img_pan, box_pan)\n",
    "        \n",
    "        #print(img_pan_cropped.shape, img_ms_cropped.shape, int_UID)\n",
    "        \n",
    "        yield int_UID, img_ms_cropped, img_pan_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiles_to_npy(n_tiles, imgs_pan, imgs_ms, int_UIDs):\n",
    "    arr_ms = np.zeros((n_tiles, MS_HEIGHT, MS_WIDTH, MS_BANDS))\n",
    "    arr_pan = np.zeros((n_tiles, PAN_HEIGHT, PAN_WIDTH, PAN_BANDS))\n",
    "    tile_img_IDs = []\n",
    "    i = 0\n",
    "    for tile_int_UID, img_ms_tile, img_pan_tile in generate_tiles(n_tiles, imgs_pan, imgs_ms, int_UIDs):\n",
    "        arr_ms[i,:,:,:] = img_ms_tile.numpy()\n",
    "        arr_pan[i,:,:,:] = img_pan_tile.numpy()\n",
    "        tile_img_IDs.append(tile_int_UID)\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            print('generated', i, 'tiles')\n",
    "    \n",
    "    return arr_ms, arr_pan, tile_img_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training tiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr_ms, arr_pan, tile_img_IDs = tiles_to_npy(2000, imgs_pan, imgs_ms, images_for_early_trials_int_UIDs)\n",
    "\n",
    "plt.imshow(arr_pan[10,:,:,0], cmap = 'gray')\n",
    "len(arr_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tiles(arr_ms, arr_pan, tile_img_IDs, path_out, prefix):\n",
    "    path_out = pathlib.Path(path_out)\n",
    "    \n",
    "    np.save(pathlib.Path(path_out, str(prefix + '-ms-n' + str(len(arr_ms)))), arr_ms)\n",
    "    print('Saved ms tiles to .npy')\n",
    "    np.save(pathlib.Path(path_out, str(prefix + '-pan-n' + str(len(arr_pan)))), arr_pan)\n",
    "    print('Saved pan tiles to .npy')\n",
    "    \n",
    "    with open(pathlib.Path(path_out, str(prefix + '-IDs-n' + str(len(arr_pan)) + '.txt')), 'w') as f:\n",
    "        for item in tile_img_IDs:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    print('Saved tile<->image ID list to .txt')          \n",
    "    \n",
    "#save_tiles(arr_ms, arr_pan, tile_img_IDs, DATA_PATH_TILES, '01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiles(dir_path, prefix):\n",
    "    dir_path = pathlib.Path(dir_path)\n",
    "    ms_path = list(dir_path.glob(str(prefix + '-ms*')))[0]\n",
    "    pan_path = list(dir_path.glob(str(prefix + '-pan*')))[0]\n",
    "    ID_path = list(dir_path.glob(str(prefix + '-ID*')))[0]\n",
    "    \n",
    "    arr_ms = np.load(ms_path)\n",
    "    arr_pan = np.load(pan_path)\n",
    "    \n",
    "    with open(ID_path, 'r') as f:\n",
    "        tile_img_IDs = f.read().splitlines()\n",
    "    tile_img_IDs = [int(ID) for ID in tile_img_IDs]\n",
    "            \n",
    "    return arr_ms, arr_pan, tile_img_IDs\n",
    "\n",
    "arr_ms, arr_pan, tile_img_IDs = load_tiles(DATA_PATH_TILES, '01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_ms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "# Check GPUs:\",\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Prevent TensorFlow from allocating all memory of all GPUs:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubic_upsampling(arr):\n",
    "    arr = tf.image.resize(arr, [PAN_HEIGHT, PAN_WIDTH], method=tf.image.ResizeMethod.BICUBIC)\n",
    "    return arr\n",
    "arr_ms_upsampled = bicubic_upsampling(arr_ms)\n",
    "arr_ms_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_srcnn(channels_in, channels_out):\n",
    "    \n",
    "    srcnn = Sequential()\n",
    "    \n",
    "    srcnn.add(Conv2D(filters=128, kernel_size = (9, 9), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None),\n",
    "                     bias_initializer='zeros',\n",
    "                     activation='relu', padding='same', use_bias=True, \n",
    "                     input_shape=(PAN_HEIGHT, PAN_WIDTH, channels_in)))\n",
    "\n",
    "    srcnn.add(Conv2D(filters=64, kernel_size = (1, 1), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None),\n",
    "                     bias_initializer='zeros',\n",
    "                     activation='relu', padding='same', use_bias=True))\n",
    "    \n",
    "    srcnn.add(Conv2D(filters=channels_out, kernel_size = (5, 5), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None), \n",
    "                     bias_initializer='zeros',\n",
    "                     activation='linear', padding='same', use_bias=True))\n",
    "    \n",
    "    # define optimizer\n",
    "    adam = Adam(lr=0.0003)\n",
    "    \n",
    "    # compile model\n",
    "    srcnn.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return srcnn\n",
    "\n",
    "srcnn = build_srcnn(channels_in = MS_BANDS, channels_out = PAN_BANDS)\n",
    "srcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = srcnn.fit(arr_ms_upsampled, arr_pan,\n",
    "                    epochs = EPOCHS, \n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    #validation_data = (lr_test, hr_test)\n",
    "                   )\n",
    "srcnn.save_weights('models/model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcnn.load_weights('models/model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_ms_upsampled.numpy()[:,:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_ms_upsampled_np = arr_ms_upsampled.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(ms, pan):\n",
    "    sr = srcnn.predict(np.expand_dims(ms, axis = 0))[0,:,:,:]\n",
    "    \n",
    "    cmap = 'gray'\n",
    "    fig = plt.figure(figsize = (80,80))\n",
    "    \n",
    "    ax0 = fig.add_subplot(1,3,1)\n",
    "    ax0.set_title('MS')\n",
    "    ax0 = plt.imshow(ms[:,:,0], cmap = cmap)\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,3,2)\n",
    "    ax1.set_title('SRCNN')\n",
    "    ax1 = plt.imshow(sr[:,:,0], cmap = cmap)\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,3,3)\n",
    "    ax2.set_title('PAN')\n",
    "    ax2 = plt.imshow(pan[:,:,0], cmap = cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idxs = np.random.randint(0, len(arr_ms_upsampled), size = 30)\n",
    "for idx in random_idxs:\n",
    "    print(arr_ms_upsampled_np[idx,:,:,:].shape)\n",
    "    plot_comparison(arr_ms_upsampled.numpy()[idx,:,:,:], arr_pan[idx,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_ms = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad below: Not runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(generate_tiles, \n",
    "                                         args=[3, imgs_pan, imgs_ms, images_for_early_trials], \n",
    "                                         output_types= tf.float32, \n",
    "                                         output_shapes = tf.TensorShape([96, 96, 8])\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(generate_tiles, \n",
    "                                         args=[3, imgs_pan, imgs_ms, images_for_early_trials], \n",
    "                                         output_types=(None , None), \n",
    "                                         output_shapes = ((96, 96, 8) , (384, 384, 1) )\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(dataset.take(10).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count_batch in dataset.repeat().batch(10).take(10):\n",
    "    print(count_batch.shape)\n",
    "    print()\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_ms, arr_pan, tile_imgID_map = preprocess_tiles(imgs_pan, imgs_ms, images_for_early_trials, 10)\n",
    "\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "    \n",
    "ax0 = fig.add_subplot(1,2,1)\n",
    "ax0.set_title('MS')\n",
    "ax0 = plt.imshow(arr_ms[0,:,:,0], cmap = 'gray')\n",
    "    \n",
    "ax1 = fig.add_subplot(1,2,2)\n",
    "ax1.set_title('PAN')\n",
    "ax1 = plt.imshow(arr_pan[0,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    feature = {\n",
    "        #'img_ID': _bytes_feature(img_ID),\n",
    "        'image_pan': _bytes_feature(img_pan_cropped),\n",
    "        'pan_height': _int64_feature(img_pan_cropped.shape[0]),\n",
    "        'pan_width': _int64_feature(img_pan_cropped.shape[1]),\n",
    "        'pan_channels': _int64_feature(img_pan_cropped.shape[2]),\n",
    "        'image_ms': _bytes_feature(img_ms_cropped),\n",
    "        'ms_height': _int64_feature(img_ms_cropped.shape[0]),\n",
    "        'ms_width': _int64_feature(img_ms_cropped.shape[1]),\n",
    "        'ms_channels': _int64_feature(img_ms_cropped.shape[2])\n",
    "    }\n",
    "    \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  }
 ],
 "metadata": {
  "keep_output": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
