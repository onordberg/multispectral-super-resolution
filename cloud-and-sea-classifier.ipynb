{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and sea tile classifier\n",
    "\n",
    "It has been identified that ESRGAN in both pretrain PSNR mode and GAN mode struggles with super-resoluting satellite image tiles completely covered by either sea surface or opaque clouds. In addition, both areas of interest, the harbors of Toulon and La Spezia, has lots of sea surface (approaching 50%). Left without intervention around 50% of tiles will only consist of sea and opaque clouds. It is assumed that this leads to an unwanted imbalance in what we want the model to be optimized to perform on.\n",
    "\n",
    "There are several ways to mitigate this imbalance. One way would be to manually draw a sea surface polygon in a GIS software and undersample tiles extracted from within this polygon. A downside of this approach is that interesting features (ships) within the sea surface polygon would also be undersampled.\n",
    "\n",
    "Another approach would be to train a cloud and sea tile classifier to detect the the unwanted tiles and discard all or a significant proportion of these tiles before training. This approach has the benefit of addressing the problem head on. The main downside of the approach is that it might be time-consuming to label tiles. However it is hypothesized that relatively little training data is needed to train a modern neural net classifier on such a *simple* classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import datetime\n",
    "import rasterio\n",
    "import rasterio.plot\n",
    "import tensorflow as tf\n",
    "\n",
    "from modules.tile_generator import *\n",
    "from modules.helpers import *\n",
    "from modules.image_utils import *\n",
    "from modules.tile_input_pipeline import *\n",
    "from modules.cloudsea_classifier import *\n",
    "\n",
    "# Check GPUs:\",\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Prevent TensorFlow from allocating all memory of all GPUs:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "tf.sysconfig.get_build_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggles whether to actually do the generation on this run\n",
    "# Be careful with setting these to True if tiles are already labelled!\n",
    "# New tiles will overwrite old tiles and labels are trash\n",
    "GENERATE_NEW_TILES = False\n",
    "CONVERT_TO_PNG = False\n",
    "CREATE_LABEL_CSV = False\n",
    "\n",
    "with open('metadata_df.pickle', 'rb') as file:\n",
    "    meta = pickle.load(file)\n",
    "# Path to location where individual satellite images are located\n",
    "DATA_PATH = 'data/toulon-laspezia/'\n",
    "DATA_PATH_TILES = 'data/toulon-laspezia-cloud-sea-classifier/'\n",
    "\n",
    "SENSORS = ['WV02', 'GE01', 'WV03_VNIR']\n",
    "AREAS = ['La_Spezia', 'Toulon']\n",
    "meta = meta.loc[meta['sensorVehicle'].isin(SENSORS)]\n",
    "meta = meta.loc[meta['area_name'].isin(AREAS)]\n",
    "\n",
    "N_IMAGES = len(meta.index)\n",
    "\n",
    "#96x96, 128x128, 196x196, 384x384 -- All tiles are squares\n",
    "TILE_SIZES = [96, 128, 196, 384]\n",
    "# number of tiles to generate at each tile size\n",
    "N_TILES = {96: 500, 128: 1000, 196: 500, 384: 500}\n",
    "N_TILES_TOTAL = sum(N_TILES.values())\n",
    "\n",
    "print(N_IMAGES)\n",
    "print(N_TILES)\n",
    "print(N_TILES_TOTAL)\n",
    "\n",
    "TRAIN_TILE_SIZE = 224\n",
    "\n",
    "PAN_OR_MS_OR_BOTH = 'pan'\n",
    "if PAN_OR_MS_OR_BOTH == 'pan':\n",
    "    TRAIN_TILE_BANDS = 1\n",
    "elif PAN_OR_MS_OR_BOTH == 'ms':\n",
    "    TRAIN_TILE_BANDS = 4\n",
    "elif PAN_OR_MS_OR_BOTH == 'both':\n",
    "    TRAIN_TILE_BANDS = 5\n",
    "\n",
    "RESIZE_METHOD = 'bilinear'\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "VAL_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate n_tiles to every image (weighted by size of image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_NEW_TILES:\n",
    "    meta = allocate_tiles(meta, by_partition=False, n_tiles_total=N_TILES[96], new_column_name='n_tiles_96')\n",
    "    meta = allocate_tiles(meta, by_partition=False, n_tiles_total=N_TILES[128], new_column_name='n_tiles_128')\n",
    "    meta = allocate_tiles(meta, by_partition=False, n_tiles_total=N_TILES[196], new_column_name='n_tiles_196')\n",
    "    meta = allocate_tiles(meta, by_partition=False, n_tiles_total=N_TILES[384], new_column_name='n_tiles_384')\n",
    "    meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tiles to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_NEW_TILES:\n",
    "    meta['n_tiles'] = 0\n",
    "    for tile_size in TILE_SIZES:\n",
    "        pathlib.Path(DATA_PATH_TILES).joinpath(str(tile_size)).mkdir()\n",
    "        tile_size_ms = int(tile_size/4)\n",
    "        meta['n_tiles'] = meta[str('n_tiles_'+str(tile_size))]\n",
    "        generate_all_tiles(meta, save_dir = str(DATA_PATH_TILES+'/'+str(tile_size)), \n",
    "                           ms_height_width=(tile_size_ms,tile_size_ms), sr_factor=4, \n",
    "                           cloud_sea_removal=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the directory structure after generation\n",
    "\n",
    "Lots of foor loops in order to do one change at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_NEW_TILES:\n",
    "    # Remove train/val/test directories\n",
    "    for tilesize_dir in pathlib.Path(DATA_PATH_TILES).iterdir():\n",
    "        for partition_dir in tilesize_dir.iterdir():\n",
    "            for image_dir in partition_dir.iterdir():\n",
    "                dest = tilesize_dir.joinpath(image_dir.stem)\n",
    "                source = image_dir\n",
    "                source.rename(dest)\n",
    "            partition_dir.rmdir()\n",
    "\n",
    "    # Add tile size to filenames\n",
    "    for tilesize_dir in pathlib.Path(DATA_PATH_TILES).iterdir():\n",
    "        for image_dir in tilesize_dir.iterdir():\n",
    "            for ms_pan_dir in image_dir.iterdir():\n",
    "                for tile in ms_pan_dir.iterdir():\n",
    "                    new_tile_name = str(tilesize_dir.stem+'-'+tile.name)\n",
    "                    new_path = ms_pan_dir.joinpath(new_tile_name)\n",
    "                    tile.rename(new_path)\n",
    "\n",
    "    # Completely flatten file structure, remove tile size directories\n",
    "    for tilesize_dir in pathlib.Path(DATA_PATH_TILES).iterdir():\n",
    "        for image_dir in tilesize_dir.iterdir():\n",
    "            for ms_pan_dir in image_dir.iterdir():\n",
    "                for tile in ms_pan_dir.iterdir():\n",
    "                    new_dir = pathlib.Path(DATA_PATH_TILES).joinpath(image_dir.stem, ms_pan_dir.name)\n",
    "                    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    new_path = new_dir.joinpath(tile.name)\n",
    "                    tile.rename(new_path)\n",
    "                ms_pan_dir.rmdir()\n",
    "            image_dir.rmdir()\n",
    "        tilesize_dir.rmdir()\n",
    "\n",
    "    # Add image_int_uid to filenames and flatten structure completely\n",
    "    for image_dir in pathlib.Path(DATA_PATH_TILES).iterdir():\n",
    "        if image_dir.stem == 'ms' or image_dir.stem == 'pan':\n",
    "            continue\n",
    "        for ms_pan_dir in image_dir.iterdir():\n",
    "            for tile in ms_pan_dir.iterdir():\n",
    "                int_uid = get_int_uid(meta, image_dir.stem)\n",
    "                new_tile_name = str(str(int_uid).zfill(2)+'-'+tile.name)\n",
    "                new_dir = pathlib.Path(DATA_PATH_TILES).joinpath(ms_pan_dir.stem)\n",
    "                new_dir.mkdir(parents=True, exist_ok=True)\n",
    "                new_path = new_dir.joinpath(new_tile_name)\n",
    "                tile.rename(new_path)\n",
    "            ms_pan_dir.rmdir()\n",
    "        image_dir.rmdir()\n",
    "\n",
    "# List all tif files\n",
    "tif_paths = [file for file in pathlib.Path(DATA_PATH_TILES).glob('**/*.tif')]\n",
    "tif_paths_ms = tif_paths[:2500]\n",
    "tif_paths_pan = tif_paths[2500:]\n",
    "\n",
    "# Divide by 2 because each tile consists of 1 MS + 1 PAN\n",
    "print('Number of tiles generated and present in flat file structure:', str(int(len(tif_paths)/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to png\n",
    "While the input to the actual cloud/sea classifier is tif files it is practical to also convert the image tiles to png. This makes labelling easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONVERT_TO_PNG:\n",
    "    for tif_path in tif_paths:\n",
    "        ms_or_pan = tif_path.parent.stem\n",
    "        \n",
    "        # sensor type is needed for conversion of ms to rgb png::\n",
    "        int_uid = int(tif_path.stem[:2])\n",
    "        string_uid = get_string_uid(meta, int_uid)\n",
    "        sensor = get_sensor(meta, string_uid)\n",
    "        \n",
    "        # saves png to disk\n",
    "        geotiff_to_png(tif_path, ms_or_pan=ms_or_pan, scale=True, stretch_img=True, sensor=sensor)\n",
    "\n",
    "# List all png files\n",
    "png_paths = [file for file in pathlib.Path(DATA_PATH_TILES).glob('**/*.png')]\n",
    "\n",
    "# Divide by 2 because each tile consists of 1 MS + 1 PAN\n",
    "print('Number of tiles generated and present in flat file structure:', str(int(len(png_paths)/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create label csv file\n",
    "Labels are `None` before manual labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_LABEL_CSV:\n",
    "    label_df = pd.DataFrame([tif_path.stem for tif_path in tif_paths[:N_TILES_TOTAL]], columns=['tile_uid'])\n",
    "    label_df['cloud-sea'] = None\n",
    "    label_df.to_csv(pathlib.Path(DATA_PATH_TILES).joinpath('labels-to-be.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling\n",
    "*... 4 tedious labelling hours later...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the labeled csv\n",
    "\n",
    "Our `load_and_populate_label_df` function also extracts `sensor`, `tile_size` and `img_uid` from the `tile_uid` column. This is used later when preparing data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = load_and_populate_label_df(DATA_PATH_TILES + '/labels.csv', meta)\n",
    "label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of images and labels\n",
    "\n",
    "Images in have different resolution. Classifier model architecture `EfficientNet` requires fixed image sizes as input so images are resized to `TRAIN_TILE_SIZE_PAN = 224`. [Native EfficientNet image sizes](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/)\n",
    "\n",
    "Note that `EfficientNet` has rescaling and normalizing layers as their first layers so such preprocessing is not required in advance. In fact it seems like such preprocessing hurts performance of the model. We can therefore keep image tiles with `dtype=uint16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_for_training(label_df, tif_paths_pan, tif_paths_ms,\n",
    "                            pan_or_ms_or_both=PAN_OR_MS_OR_BOTH,\n",
    "                            pan_tile_size=TRAIN_TILE_SIZE, ms_tile_size=TRAIN_TILE_SIZE, \n",
    "                            resize_method=RESIZE_METHOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(augment=True, input_shape=(TRAIN_TILE_SIZE, TRAIN_TILE_SIZE, TRAIN_TILE_BANDS))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model_name = str('cloud-sea-classifier-effnetb0-pan-augm')\n",
    "log_dir = pathlib.Path(str('logs/cloud-sea-classifier/fit/' + pretrain_model_name \n",
    "                           + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                      histogram_freq=10, \n",
    "                                                      write_graph=False, \n",
    "                                                      write_images=False,\n",
    "                                                      update_freq='epoch')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = str('models/cloud-sea-classifier/' + pretrain_model_name + '-{epoch:02d}-{val_loss:.6f}.h5'), \n",
    "    monitor = \"val_acc\",\n",
    "    save_best_only = False,\n",
    "    save_weights_only = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          validation_split=VAL_SPLIT, initial_epoch=0,\n",
    "          callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZE_METHODS = ['bilinear', 'nearest', 'bicubic']\n",
    "PAN_MS_BOTH = ['pan', 'ms', 'both']\n",
    "LEARNING_RATES = [0.001, 0.0005, 0.0001]\n",
    "EPOCHS = 100\n",
    "\n",
    "for resize_method in RESIZE_METHODS:\n",
    "    for pan_ms_both in PAN_MS_BOTH:\n",
    "        for learning_rate in LEARNING_RATES:\n",
    "\n",
    "            X, y = prepare_for_training(label_df, tif_paths_pan, tif_paths_ms,\n",
    "                                        pan_or_ms_or_both=pan_ms_both,\n",
    "                                        pan_tile_size=TRAIN_TILE_SIZE, ms_tile_size=TRAIN_TILE_SIZE, \n",
    "                                        resize_method=resize_method)\n",
    "\n",
    "            model = build_model(augment=True, input_shape=(TRAIN_TILE_SIZE, TRAIN_TILE_SIZE, TRAIN_TILE_BANDS))\n",
    "\n",
    "            pretrain_model_name = str('cloudsea-effb0-augm-' + resize_method \n",
    "                                      + '-' + pan_ms_both + '-' + str(learning_rate) + '-')\n",
    "            log_dir = pathlib.Path(str('logs/cloud-sea-classifier/fit/' + pretrain_model_name \n",
    "                                       + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')))\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                                  histogram_freq=10, \n",
    "                                                                  write_graph=False, \n",
    "                                                                  write_images=False,\n",
    "                                                                  update_freq='epoch')\n",
    "\n",
    "            checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath = str('models/cloud-sea-classifier/' + pretrain_model_name + '-{epoch:02d}-{val_loss:.6f}.h5'), \n",
    "                monitor = \"val_acc\",\n",
    "                save_best_only = False,\n",
    "                save_weights_only = True,\n",
    "                )\n",
    "\n",
    "            model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                      validation_split=VAL_SPLIT, initial_epoch=0,\n",
    "                      callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
