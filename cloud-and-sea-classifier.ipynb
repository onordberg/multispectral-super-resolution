{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and sea tile classifier\n",
    "\n",
    "It has been identified that ESRGAN in both pretrain PSNR mode and GAN mode struggles with super-resoluting satellite image tiles completely covered by either sea surface or opaque clouds. In addition, both areas of interest, the harbors of Toulon and La Spezia, has lots of sea surface (approaching 50%). Left without intervention around 50% of tiles will only consist of sea and opaque clouds. It is assumed that this leads to an unwanted imbalance in what we want the model to be optimized to perform on.\n",
    "\n",
    "There are several ways to mitigate this imbalance. One way would be to manually draw a sea surface polygon in a GIS software and undersample tiles extracted from within this polygon. A downside of this approach is that interesting features (ships) within the sea surface polygon would also be undersampled.\n",
    "\n",
    "Another approach would be to train a cloud and sea tile classifier to detect the the unwanted tiles and discard all or a significant proportion of these tiles before training. This approach has the benefit of addressing the problem head on. The main downside of the approach is that it might be time-consuming to label tiles. However it is hypothesized that relatively little training data is needed to train a modern neural net classifier on such a *simple* classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import rasterio\n",
    "import rasterio.plot\n",
    "import tensorflow as tf\n",
    "\n",
    "from modules.tile_generator import *\n",
    "from modules.helpers import *\n",
    "from modules.image_utils import *\n",
    "from modules.tile_input_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggles whether to actually do the generation on this run\n",
    "# Be careful with setting these to True if tiles are already labelled!\n",
    "# New tiles will overwrite old tiles and labels are trash\n",
    "GENERATE_NEW_TILES = False\n",
    "CONVERT_TO_PNG = False\n",
    "CREATE_LABEL_CSV = False\n",
    "\n",
    "with open('metadata_df.pickle', 'rb') as file:\n",
    "    meta = pickle.load(file)\n",
    "# Path to location where individual satellite images are located\n",
    "DATA_PATH = 'data/toulon-laspezia/'\n",
    "DATA_PATH_TILES = 'data/toulon-laspezia-cloud-sea-classifier/'\n",
    "\n",
    "SENSORS = ['WV02', 'GE01', 'WV03_VNIR']\n",
    "AREAS = ['La_Spezia', 'Toulon']\n",
    "meta = meta.loc[meta['sensorVehicle'].isin(SENSORS)]\n",
    "meta = meta.loc[meta['area_name'].isin(AREAS)]\n",
    "\n",
    "N_IMAGES = len(meta.index)\n",
    "\n",
    "#96x96, 128x128, 196x196, 384x384 -- All tiles are squares\n",
    "TILE_SIZES = [96, 128, 196, 384]\n",
    "# number of tiles to generate at each tile size\n",
    "N_TILES = {96: 500, 128: 1000, 196: 500, 384: 500}\n",
    "N_TILES_TOTAL = sum(N_TILES.values())\n",
    "\n",
    "print(N_IMAGES)\n",
    "print(N_TILES)\n",
    "print(N_TILES_TOTAL)\n",
    "\n",
    "TRAIN_TILE_SIZE_PAN = 128\n",
    "TRAIN_TILE_SIZE_MS = int(128/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate n_tiles to every image (weighted by size of image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_NEW_TILES:\n",
    "    meta = allocate_tiles(meta, by_partition=False, n_tiles_total=N_TILES[96], new_column_name='n_tiles_96')\n",
    "    meta = allocate_tiles(meta, by_partition=False, n_tiles_total=N_TILES[128], new_column_name='n_tiles_128')\n",
    "    meta = allocate_tiles(meta, by_partition=False, n_tiles_total=N_TILES[196], new_column_name='n_tiles_196')\n",
    "    meta = allocate_tiles(meta, by_partition=False, n_tiles_total=N_TILES[384], new_column_name='n_tiles_384')\n",
    "    meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tiles to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_NEW_TILES:\n",
    "    meta['n_tiles'] = 0\n",
    "    for tile_size in TILE_SIZES:\n",
    "        pathlib.Path(DATA_PATH_TILES).joinpath(str(tile_size)).mkdir()\n",
    "        tile_size_ms = int(tile_size/4)\n",
    "        meta['n_tiles'] = meta[str('n_tiles_'+str(tile_size))]\n",
    "        generate_all_tiles(meta, save_dir = str(DATA_PATH_TILES+'/'+str(tile_size)), \n",
    "                           ms_height_width=(tile_size_ms,tile_size_ms), sr_factor=4, \n",
    "                           cloud_sea_removal=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the directory structure after generation\n",
    "\n",
    "Lots of foor loops in order to do one change at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_NEW_TILES:\n",
    "    # Remove train/val/test directories\n",
    "    for tilesize_dir in pathlib.Path(DATA_PATH_TILES).iterdir():\n",
    "        for partition_dir in tilesize_dir.iterdir():\n",
    "            for image_dir in partition_dir.iterdir():\n",
    "                dest = tilesize_dir.joinpath(image_dir.stem)\n",
    "                source = image_dir\n",
    "                source.rename(dest)\n",
    "            partition_dir.rmdir()\n",
    "\n",
    "    # Add tile size to filenames\n",
    "    for tilesize_dir in pathlib.Path(DATA_PATH_TILES).iterdir():\n",
    "        for image_dir in tilesize_dir.iterdir():\n",
    "            for ms_pan_dir in image_dir.iterdir():\n",
    "                for tile in ms_pan_dir.iterdir():\n",
    "                    new_tile_name = str(tilesize_dir.stem+'-'+tile.name)\n",
    "                    new_path = ms_pan_dir.joinpath(new_tile_name)\n",
    "                    tile.rename(new_path)\n",
    "\n",
    "    # Completely flatten file structure, remove tile size directories\n",
    "    for tilesize_dir in pathlib.Path(DATA_PATH_TILES).iterdir():\n",
    "        for image_dir in tilesize_dir.iterdir():\n",
    "            for ms_pan_dir in image_dir.iterdir():\n",
    "                for tile in ms_pan_dir.iterdir():\n",
    "                    new_dir = pathlib.Path(DATA_PATH_TILES).joinpath(image_dir.stem, ms_pan_dir.name)\n",
    "                    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    new_path = new_dir.joinpath(tile.name)\n",
    "                    tile.rename(new_path)\n",
    "                ms_pan_dir.rmdir()\n",
    "            image_dir.rmdir()\n",
    "        tilesize_dir.rmdir()\n",
    "\n",
    "    # Add image_int_uid to filenames and flatten structure completely\n",
    "    for image_dir in pathlib.Path(DATA_PATH_TILES).iterdir():\n",
    "        if image_dir.stem == 'ms' or image_dir.stem == 'pan':\n",
    "            continue\n",
    "        for ms_pan_dir in image_dir.iterdir():\n",
    "            for tile in ms_pan_dir.iterdir():\n",
    "                int_uid = get_int_uid(meta, image_dir.stem)\n",
    "                new_tile_name = str(str(int_uid).zfill(2)+'-'+tile.name)\n",
    "                new_dir = pathlib.Path(DATA_PATH_TILES).joinpath(ms_pan_dir.stem)\n",
    "                new_dir.mkdir(parents=True, exist_ok=True)\n",
    "                new_path = new_dir.joinpath(new_tile_name)\n",
    "                tile.rename(new_path)\n",
    "            ms_pan_dir.rmdir()\n",
    "        image_dir.rmdir()\n",
    "\n",
    "# List all tif files\n",
    "tif_paths = [file for file in pathlib.Path(DATA_PATH_TILES).glob('**/*.tif')]\n",
    "tif_paths_ms = tif_paths[:2500]\n",
    "tif_paths_pan = tif_paths[2500:]\n",
    "\n",
    "# Divide by 2 because each tile consists of 1 MS + 1 PAN\n",
    "print('Number of tiles generated and present in flat file structure:', str(int(len(tif_paths)/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to png\n",
    "While the input to the actual cloud/sea classifier is tif files it is practical to also convert the image tiles to png. This makes labelling easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONVERT_TO_PNG:\n",
    "    for tif_path in tif_paths:\n",
    "        ms_or_pan = tif_path.parent.stem\n",
    "        \n",
    "        # sensor type is needed for conversion of ms to rgb png::\n",
    "        int_uid = int(tif_path.stem[:2])\n",
    "        string_uid = get_string_uid(meta, int_uid)\n",
    "        sensor = get_sensor(meta, string_uid)\n",
    "        \n",
    "        # saves png to disk\n",
    "        geotiff_to_png(tif_path, ms_or_pan=ms_or_pan, scale=True, stretch_img=True, sensor=sensor)\n",
    "\n",
    "# List all png files\n",
    "png_paths = [file for file in pathlib.Path(DATA_PATH_TILES).glob('**/*.png')]\n",
    "\n",
    "# Divide by 2 because each tile consists of 1 MS + 1 PAN\n",
    "print('Number of tiles generated and present in flat file structure:', str(int(len(png_paths)/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create label csv file\n",
    "Labels are `None` before manual labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_LABEL_CSV:\n",
    "    label_df = pd.DataFrame([tif_path.stem for tif_path in tif_paths[:N_TILES_TOTAL]], columns=['tile_uid'])\n",
    "    label_df['cloud-sea'] = None\n",
    "    label_df.to_csv(pathlib.Path(DATA_PATH_TILES).joinpath('labels-to-be.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling\n",
    "*... 4 tedious labelling hours later...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the labels from csv\n",
    "label_df = pd.read_csv(pathlib.Path(DATA_PATH_TILES).joinpath('labels.csv'), delimiter=';', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df['cloud-sea'].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking that UIDs match\n",
    "If the sequence of image tiles and labels in `tif_paths_pan`, `tif_paths_ms` and `label_df` match we can use integer indices instead of string UIDs when training (quicker to code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_x_y_match(tif_paths_pan, tif_paths_ms, label_df):\n",
    "    try:\n",
    "        assert len(label_df) == len(tif_paths_pan) == len(tif_paths_ms)\n",
    "    except AssertionError:\n",
    "        print('Lengths of tif paths and label dataframe differ!')\n",
    "        \n",
    "    n = len(label_df)\n",
    "    try:\n",
    "        for i in range(n):\n",
    "            label_tile_uid = label_df.iloc[i]['tile_uid']\n",
    "            tif_tile_ms_uid = tif_paths_ms[i].stem\n",
    "            tif_tile_pan_uid = tif_paths_pan[i].stem\n",
    "            #print(label_tile_uid, tif_tile_ms_uid, tif_tile_pan_uid)\n",
    "            assert label_tile_uid == tif_tile_ms_uid == tif_tile_pan_uid\n",
    "    except AssertionError:\n",
    "        print('Mismatch between sequence of tile uids!')\n",
    "        print('label_tile_uid:', label_tile_uid)\n",
    "        print('tif_tile_ms_uid', tif_tile_ms_uid)\n",
    "        print('tif_tile_pan_uid', tif_tile_pan_uid)\n",
    "    print('Verification OK. All', n, 'image tile UIDs match.')\n",
    "        \n",
    "assert_x_y_match(tif_paths_pan, tif_paths_ms, label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(tif_paths, label_df, tile_size, pan_or_ms_or_both='pan'):\n",
    "    n = len(label_df)\n",
    "    y = label_df['cloud-sea'].to_numpy(dtype=np.float32)\n",
    "    if pan_or_ms_or_both == 'pan':\n",
    "        X = np.empty((n, tile_size, tile_size, 1), dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        img = geotiff_to_ndarray(tif_paths[i])\n",
    "        #print(img.shape)\n",
    "        img = tf.image.resize(img, [tile_size, tile_size], method=tf.image.ResizeMethod.BILINEAR)\n",
    "        X[i,:,:,:] = tf.image.convert_image_dtype(img, tf.float32).numpy()\n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_for_training(tif_paths_pan, label_df, tile_size=TRAIN_TILE_SIZE_PAN, pan_or_ms_or_both='pan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
