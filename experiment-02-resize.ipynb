{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 02: Resize to 2.0/0.5 m\n",
    "\n",
    "In this experiment we will study how different band configurations affect the training process and end results.\n",
    "\n",
    "### Experiment variations:\n",
    "\n",
    "- E01-8: All 8 MS WorldView-2 bands is used.\n",
    "- E01-6: The 6 MS WorldView-2 bands that overlap the PAN band (RGB+Y+RedEdge+NIR1)\n",
    "- E01-4: The 4 MS WorldView-2 bands that are also available in GeoEye-1 (MS1 array RGB+NIR1)\n",
    " - In this variation we will also validate performance on the GeoEye-1 validation set\n",
    "- E01-3: Only the 3 RGB bands from WorldView-2\n",
    " - Also validated on the GeoEye-1 validation set\n",
    " \n",
    "### The notebook is divided into the following main sections:\n",
    "1. Imports and configuration parameters\n",
    "2. Tile generation (sampling of tiles from the satellite images)\n",
    "3. Tile input pipelines (`tf.dataset` objects reading tiles from disk)\n",
    "4. Building of models\n",
    "5. Pretraining with L1 loss\n",
    "6. Build the full ESRGAN model\n",
    "7. GAN-training with L1 + Percep + GAN loss\n",
    "8. Inspection of results\n",
    "\n",
    "Training history is logged with TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bc9308e2e1d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile_generator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatlab_metrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile_input_pipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ono008\\master\\multispectral-super-resolution\\modules\\matlab_metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mskvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\skvideo\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"1.1.10\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\skvideo\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0medge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcanny\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstpyr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpatialSteerablePyramid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmscn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_image_mscn_transform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_gauss_window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mggd_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggd_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaired_product\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\skvideo\\utils\\stpyr.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\scipy\\signal\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mspectral\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwavelets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_peak_finding\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwindows\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_window\u001b[0m  \u001b[1;31m# keep this one in signal namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\scipy\\signal\\_peak_finding.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwavelets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcwt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mricker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscoreatpercentile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m from ._peak_finding_utils import (\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \"\"\"\n\u001b[1;32m--> 391\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\scipy\\stats\\distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                     rv_frozen)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_discrete_distns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   4818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4820\u001b[1;33m \u001b[0mlevy_stable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlevy_stable_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'levy_stable'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, extradoc, seed)\u001b[0m\n\u001b[0;32m   1701\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m                 \u001b[0mdct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistcont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1703\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ono008\\master\\multispectral-super-resolution\\env\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m_construct_doc\u001b[1;34m(self, docdict, shapes_vals)\u001b[0m\n\u001b[0;32m    794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%(shapes)s, \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoccer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtempdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m                 raise Exception(\"Unable to construct docstring for distribution \\\"%s\\\": %s\" %\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from modules.helpers import *\n",
    "from modules.tile_generator import *\n",
    "from modules.matlab_metrics import *\n",
    "from modules.image_utils import *\n",
    "from modules.tile_input_pipeline import *\n",
    "from modules.models import *\n",
    "from modules.evaluation import *\n",
    "\n",
    "from modules.logging import *\n",
    "from modules.train import *\n",
    "\n",
    "import time\n",
    "\n",
    "# Check GPUs and enable dynamic GPU memory use:\",\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Prevent TensorFlow from allocating all memory of all GPUs:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SETTINGS ###############################################################################################\n",
    "EXPERIMENT_NAMES = ['e02-4', 'e02-3']\n",
    "\n",
    "# Select experiment variation to be run in THIS notebook:\n",
    "EXPERIMENT = EXPERIMENT_NAMES[0]\n",
    "\n",
    "# Turn on and off certain time consuming processes in the notebook:\n",
    "RESIZE_TO_PIXEL_SIZE = False\n",
    "GENERATE_TILES = False   # This should only be done once in experiment 01. All variations will read from the same\n",
    "TILE_DENSITY_MAPS = False  # Loops through all tiles and compute density maps of where tiles have been sampled\n",
    "CALCULATE_STATS = False  # Loops through all tiles and calculate mean and sd. Used for scaling\n",
    "PRE_BUILD = True          # Step 1 of the training process\n",
    "PRETRAIN = True          # Step 1 of the training process\n",
    "GAN_BUILD = False          # Step 1 of the training process\n",
    "GAN_TRAIN = False         # Step 2 of the training process\n",
    "PRE_EVALUATE_LAST = False\n",
    "GAN_EVALUATE_LAST = False\n",
    "PRE_EVALUATE_HISTORY = False\n",
    "GAN_EVALUATE_HISTORY = False\n",
    "\n",
    "# Load metadata dataframe \"meta\" from repository root. \n",
    "# This dataframe keeops track of images and is used and updated throughout the notebook\n",
    "meta = load_meta_pickle_csv('.', 'metadata_df', from_pickle=True)\n",
    "#################################################################################################################\n",
    "\n",
    "### PATHS #######################################################################################################\n",
    "DATA_PATH = 'data/toulon-laspezia'\n",
    "DATA_PATH_TILES = 'data/toulon-laspezia-tiles/e02'\n",
    "DATA_PATH_TILES_P = {'train': DATA_PATH_TILES + '/train', \n",
    "                     'val': DATA_PATH_TILES + '/val', \n",
    "                     'test': DATA_PATH_TILES + '/test'}\n",
    "LOGS_DIR = 'logs/' # Path to tensorboard logs and model checkpoint saves\n",
    "LOGS_EXP_DIR = LOGS_DIR + EXPERIMENT\n",
    "#################################################################################################################\n",
    "\n",
    "### TILE GENERATION #############################################################################################\n",
    "SENSORS_GENERATE = ['WV02', 'GE01']\n",
    "AREAS_GENERATE = ['La_Spezia', 'Toulon']\n",
    "meta = subset_by_areas_sensor(meta, areas=AREAS_GENERATE, sensors=SENSORS_GENERATE)\n",
    "print('Sensors to generate tiles from:', SENSORS_GENERATE)\n",
    "print('Areas to generate tiles from:', AREAS_GENERATE)\n",
    "\n",
    "# Count images in partitions (train/val/test):\n",
    "N_IMAGES_TOTAL = count_images(meta)\n",
    "N_IMAGES = {'train': count_images_in_partition(meta, 'train'), \n",
    "            'val': count_images_in_partition(meta, 'val'), \n",
    "            'test': count_images_in_partition(meta, 'test')}\n",
    "assert N_IMAGES_TOTAL == sum(N_IMAGES.values())  # Verify that different ways of counting adds up\n",
    "print('Number of images in partitions', N_IMAGES)\n",
    "print('Total number of images:', N_IMAGES_TOTAL)\n",
    "\n",
    "TILES_PER_M2 = {'train': 2.0, \n",
    "                'val': 2.0, \n",
    "                'test': 2.0}\n",
    "\n",
    "# Settings for whether to send every tile generated through a sea and cloud classifier\n",
    "# This is useful if images consist of a lot of sea and clouds and you want to reduce the number of tiles\n",
    "# with such monotone and less meaningful content. Classifier is trained on 2500 labeled tiles of various sizes\n",
    "# where only tiles COMPLETELY covered by sea and/or clouds have been labelled \"cloud/sea\". \n",
    "# Validation accuracy around 0.95\n",
    "CLOUD_SEA_REMOVAL = True\n",
    "CLOUD_SEA_WEIGHTS_PATH = 'models/cloud-sea-classifier/cloudsea-effb0-augm-bicubic-pan-0.0005--200-0.127841.h5'\n",
    "# Cutoff at inference time. Tiles with (quasi)-prob higher than cutoff will be classified as cloud and or sea:\n",
    "CLOUD_SEA_PRED_CUTOFF = 0.95\n",
    "# Setting to keep a certain proportion of cloud/sea tiles through the filter:\n",
    "CLOUD_SEA_KEEP_RATE = 0.10\n",
    "\n",
    "### RESIZING ####################################################################################################\n",
    "RESIZE_RESAMPLING_METHOD = 'nearest'  # 'nearest', 'bicubic', 'bilinear'\n",
    "NEW_PIXEL_SIZE_PAN = 0.5\n",
    "RESIZE_DIR = DATA_PATH + '-resized'\n",
    "#################################################################################################################\n",
    "\n",
    "### SENSORS AND AREA EXPERIMENT SELECTION #######################################################################\n",
    "# Sensors used in which experiment variation\n",
    "SENSORS_EXP = {'e02-4': {'train': 'WV02', 'val': ['WV02', 'GE01'], 'test': ['WV02', 'GE01']}, \n",
    "               'e02-3': {'train': 'WV02', 'val': ['WV02', 'GE01'], 'test': ['WV02', 'GE01']}}\n",
    "SENSORS = SENSORS_EXP[EXPERIMENT]\n",
    "\n",
    "# Areas used in which experiment variation\n",
    "AREAS_EXP = {'e02-4': {'train': AREAS_GENERATE, 'val': AREAS_GENERATE, 'test': AREAS_GENERATE}, \n",
    "             'e02-3': {'train': AREAS_GENERATE, 'val': AREAS_GENERATE, 'test': AREAS_GENERATE}}\n",
    "AREAS = AREAS_EXP[EXPERIMENT]\n",
    "#################################################################################################################\n",
    "\n",
    "### TILE DIMENSIONS #############################################################################################\n",
    "# Note larger size of val and test. This is needed for sensible calculation of Ma, NIQE and PI calculation\n",
    "SR_FACTOR = 4\n",
    "MS_SIZE = {'train': 32, 'val': 128, 'test': 128}\n",
    "PAN_SIZE = {'train': MS_SIZE['train'] * SR_FACTOR, \n",
    "            'val': MS_SIZE['val'] * SR_FACTOR, \n",
    "            'test': MS_SIZE['test'] * SR_FACTOR}\n",
    "print('MS (LR) tile size:', MS_SIZE)\n",
    "print('PAN (HR) tile size:', PAN_SIZE)\n",
    "print('SR factor:', SR_FACTOR)\n",
    "#################################################################################################################\n",
    "\n",
    "### BAND (CHANNEL) CONFIGURATIONS ###############################################################################\n",
    "# This is the essence of experiment 01\n",
    "# Selection of bands is done in the tile input pipeline\n",
    "\n",
    "# Selecting bands from the 8 bands of WV02:\n",
    "WV02_FULL_BAND_CONFIG = get_sensor_bands('WV02', meta)\n",
    "WV02_EXP_BAND_CONFIGS = {'e02-4': {k:v for (k,v) in WV02_FULL_BAND_CONFIG.items()  # 4 bands (BGR+NIR)\n",
    "                                   if k in ['Blue', 'Green', 'Red', 'NIR']},\n",
    "                         'e02-3': {k:v for (k,v) in WV02_FULL_BAND_CONFIG.items()  # 3 bands (BGR)\n",
    "                                   if k in ['Blue', 'Green', 'Red']}}\n",
    "MS_BANDS_WV02_CONFIG = WV02_EXP_BAND_CONFIGS[EXPERIMENT]\n",
    "MS_BANDS_WV02_IDXS = list(MS_BANDS_WV02_CONFIG.values())\n",
    "\n",
    "N_MS_BANDS = len(MS_BANDS_WV02_CONFIG.values()) # The number of MS bands in this experiment variation\n",
    "\n",
    "# Selecting bands from the 4 bands of GE01:\n",
    "GE01_FULL_BAND_CONFIG = get_sensor_bands('GE01', meta)                            \n",
    "GE01_EXP_BAND_CONFIGS = {'e02-4': GE01_FULL_BAND_CONFIG,                           # 4 (all) bands (BGR+NIR)\n",
    "                         'e02-3': {k:v for (k,v) in GE01_FULL_BAND_CONFIG.items()  # 3 bands (BGR)\n",
    "                                   if k not in ['NIR']}}\n",
    "MS_BANDS_GE01_CONFIG = GE01_EXP_BAND_CONFIGS[EXPERIMENT]\n",
    "if EXPERIMENT == 'e02-4':\n",
    "    MS_BANDS_GE01_IDXS = 'all'\n",
    "else:\n",
    "    MS_BANDS_GE01_IDXS = list(MS_BANDS_GE01_CONFIG.values())\n",
    "print('MS (LR) Band Config WV02:', MS_BANDS_WV02_CONFIG)\n",
    "print('MS (LR) Band Config GE01:', MS_BANDS_GE01_CONFIG)\n",
    "\n",
    "N_PAN_BANDS = 1 # Obviously only 1 panchromatic band\n",
    "#################################################################################################################\n",
    "\n",
    "### MODEL PARAMETERS ############################################################################################\n",
    "BATCH_SIZE = {'train': 16, 'val': 8, 'test': 8}\n",
    "print('Batch sizes:', BATCH_SIZE)\n",
    "\n",
    "# RRDB Generator Model parameters \n",
    "N_BLOCKS = 16 # 23 - Deeper means potential to capture more complex relationships, at the cost of training time\n",
    "N_FILTERS = 64 # Baseline setting that is not tinkered with in this repository\n",
    "#################################################################################################################\n",
    "\n",
    "### PRETRAINING SETTINGS ########################################################################################\n",
    "PRE_EPOCHS = 400\n",
    "PRE_TRAIN_STEPS = 1000  # per epoch\n",
    "PRE_VAL_STEPS = 0     # per epoch\n",
    "print('Pretraining - Total steps:', PRE_EPOCHS * PRE_TRAIN_STEPS)\n",
    "\n",
    "# Number of batches to save every epoch in TensorBoard\n",
    "TRAIN_N_BATCHES_SAVE = 2\n",
    "VAL_N_BATCHES_SAVE = 2\n",
    "\n",
    "# Optimizer settings:\n",
    "PRETRAIN_LOSS = 'l1'    # Official\n",
    "PRETRAIN_LR = 5e-5      # Tuned and found stable for this particular experiment\n",
    "#PRETRAIN_LR = 0.0002   # Official\n",
    "PRETRAIN_BETA_1 = 0.9   # Official\n",
    "PRETRAIN_BETA_2 = 0.999 # Official\n",
    "# Note: Official implementation also uses stepwise learning rate scheduler. \n",
    "# This is avoided here as it is deemed not central to the experiment to \"squeeze\" out last performance and it \n",
    "# complicates comparisons between experiment variations\n",
    "#################################################################################################################\n",
    "\n",
    "### GAN TRAINING SETTINGS #######################################################################################\n",
    "GAN_EPOCHS = 400\n",
    "GAN_TRAIN_STEPS = 1000\n",
    "GAN_VAL_STEPS = 0\n",
    "# Proportion of val batches that will go through ma and niqe metric calculation\n",
    "# MA_NIQE_PROPORTION = 0.04  # The calculation is very time consuming\n",
    "MA_NIQE_PROPORTION = 1  # The calculation is very time consuming\n",
    "print('GAN training - Total steps:', GAN_EPOCHS * GAN_TRAIN_STEPS)\n",
    "\n",
    "# Weights for each loss in the composite loss function\n",
    "G_LOSS_PIXEL_W = 0.01       # Official\n",
    "G_LOSS_PERCEP_W = 1.0       # Official\n",
    "G_LOSS_GENERATOR_W = 0.005  # Official\n",
    "\n",
    "# Optimizer settings:\n",
    "#GAN_G_LR = 1e-4 # Official\n",
    "#GAN_D_LR = 1e-4 # Official\n",
    "GAN_G_LR = 2e-5\n",
    "GAN_D_LR = 2e-5\n",
    "G_BETA_1, D_BETA_1 = 0.9, 0.9      # Official\n",
    "G_BETA_2, D_BETA_2 = 0.999, 0.999  # Official\n",
    "# Note: Official implementation also uses stepwise learning rate scheduler. \n",
    "# This is avoided here as it is deemed not central to the experiment to \"squeeze\" out last performance and it \n",
    "# complicates comparisons between experiment variations\n",
    "\n",
    "# Path to the pretraining weights that is the starting point of GAN training:\n",
    "#PRETRAIN_WEIGHTS_DIRS = {'e01-8': LOGS_EXP_DIR + '/models/' + 'e01-8-pre_20210116-194500/', \n",
    "#                         'e01-6': LOGS_EXP_DIR + '/models/' + 'e01-6-pre_20210122-091421/', \n",
    "#                         'e01-4': LOGS_EXP_DIR + '/models/' + 'e01-4-pre_20210119-101939/', \n",
    "#                         'e01-3': LOGS_EXP_DIR + '/models/' + 'e01-3-pre_20210124-153415/'\n",
    "#                        }\n",
    "#PRETRAIN_WEIGHTS_DIR = PRETRAIN_WEIGHTS_DIRS[EXPERIMENT]\n",
    "#PRETRAIN_WEIGHTS_PATH = PRETRAIN_WEIGHTS_DIR + EXPERIMENT + '-pre-400.h5'\n",
    "#\n",
    "## Path to the gan-training weights that will be \n",
    "#GAN_WEIGHTS_DIRS = {'e01-8': LOGS_EXP_DIR + '/models/' + 'e01-8-gan_20210222-124759/', \n",
    "#                    'e01-6': LOGS_EXP_DIR + '/models/' + 'e01-6-gan_20210216-152032/', \n",
    "#                    'e01-4': LOGS_EXP_DIR + '/models/' + 'e01-4-gan_20210212-144735/', \n",
    "#                    'e01-3': LOGS_EXP_DIR + '/models/' + 'e01-3-gan_20210219-135014/'\n",
    "#                   }\n",
    "#GAN_WEIGHTS_DIR = GAN_WEIGHTS_DIRS[EXPERIMENT]\n",
    "#GAN_WEIGHTS_PATH = GAN_WEIGHTS_DIR + EXPERIMENT + '-gan-G-399.h5'\n",
    "#################################################################################################################\n",
    "\n",
    "### MATLAB METRICS ##############################################################################################\n",
    "# Calculate Ma, NIQE and Perceptual Index (PI) metrics on the validation set(s) during GAN training:\n",
    "# PI was metric used in PIRM2018 competition https://github.com/roimehrez/PIRM2018\n",
    "METRIC_MA = False\n",
    "METRIC_NIQE = False\n",
    "if METRIC_MA and METRIC_NIQE:\n",
    "    METRIC_PI = True\n",
    "else:\n",
    "    METRIC_PI = False\n",
    "\n",
    "# The number of pixels to be shaved off the border of the tile before calculating Ma/NIQE/PI (ignore border effects)\n",
    "SHAVE_WIDTH = 4 # Official (as used in PIRM2018 evaluation)\n",
    "# Ma/NIQE/PI calculation is done with official matlab repositories through MATLAB Engine API for Python\n",
    "MATLAB_PATH = 'modules/matlab' # path to repositories\n",
    "#################################################################################################################\n",
    "\n",
    "### EVALUTAION ##################################################################################################\n",
    "if PRE_EVALUATE_LAST or GAN_EVALUATE_LAST:\n",
    "    METRIC_MA = False\n",
    "    METRIC_NIQE = True\n",
    "    if METRIC_MA and METRIC_NIQE:\n",
    "        METRIC_PI = True\n",
    "    else:\n",
    "        METRIC_PI = False\n",
    "        \n",
    "if PRE_EVALUATE_HISTORY and GAN_EVALUATE_HISTORY:\n",
    "    raise ValueError\n",
    "\n",
    "EVAL_STEPS_PER_EPOCH = 2000\n",
    "EVAL_N_EPOCHS = 400\n",
    "EVAL_SENSOR = 'WV02'\n",
    "EVAL_PER_IMAGE = True\n",
    "    \n",
    "if PRE_EVALUATE_HISTORY:\n",
    "    EVAL_WEIGHTS_DIR = PRETRAIN_WEIGHTS_DIR\n",
    "    EVAL_FIRST_STEP = 1\n",
    "    EVAL_PREFIX = EXPERIMENT + '-pre-'\n",
    "elif GAN_EVALUATE_HISTORY:\n",
    "    EVAL_WEIGHTS_DIR = GAN_WEIGHTS_DIR\n",
    "    EVAL_FIRST_STEP = 0\n",
    "    EVAL_PREFIX = EXPERIMENT + '-gan-'\n",
    "    \n",
    "print('MATLAB Metrics:')\n",
    "print('Ma:', METRIC_MA)\n",
    "print('NIQE:', METRIC_NIQE)\n",
    "print('Perceptual Index (PI):', METRIC_PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tile generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Image resizing\n",
    "\n",
    "Function `resize_sat_img_to_new_pixel_size` available in `modules.tile_generator`. Not used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if RESIZE_TO_PIXEL_SIZE:\n",
    "    meta = resize_all_sat_imgs_to_new_pixel_size(meta, \n",
    "                                                 save_dir=RESIZE_DIR, \n",
    "                                                 new_pixel_size_pan=(NEW_PIXEL_SIZE_PAN, NEW_PIXEL_SIZE_PAN),\n",
    "                                                 sr_factor=SR_FACTOR, resampling='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tile allocation\n",
    "\n",
    "We allocate `n_tiles` to each satellite image in proportion to the area covered by the satellite image. We adjust `n_tiles` by the argument `tiles_per_m2`. If `tiles_per_m2=1.0` then `n_tiles` is set deterministically to a value so that a square meter of satellite image is expected to be covered by `1.0` tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TILES:\n",
    "    meta = allocate_tiles_by_expected(meta, \n",
    "                                      override_pan_pixel_size=NEW_PIXEL_SIZE_PAN,\n",
    "                                      by_partition=True, \n",
    "                                      tiles_per_m2_train_val_test=(TILES_PER_M2['train'], \n",
    "                                                                   TILES_PER_M2['val'], \n",
    "                                                                   TILES_PER_M2['test']),\n",
    "                                      pan_tile_size_train_val_test=(PAN_SIZE['train'], \n",
    "                                                                    PAN_SIZE['val'], \n",
    "                                                                    PAN_SIZE['test']),\n",
    "                                      new_column_name='n_tiles')\n",
    "else:\n",
    "    # Load meta dataframe that was updated at tile generation time\n",
    "    meta = load_meta_pickle_csv(DATA_PATH_TILES, 'metadata_tile_allocation', from_pickle=True)\n",
    "\n",
    "n_tiles = {'train': count_tiles_in_partition(meta, 'train'),\n",
    "           'val': count_tiles_in_partition(meta, 'val'), \n",
    "           'test':  count_tiles_in_partition(meta, 'test')}\n",
    "n_tiles_total = count_tiles(meta)\n",
    "assert n_tiles_total == sum(n_tiles.values())\n",
    "print('Number of tiles per partition:')\n",
    "print(n_tiles)\n",
    "print('Total number of tiles:', n_tiles_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tile generation to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TILES:\n",
    "    meta = generate_all_tiles(meta, \n",
    "                              save_dir=DATA_PATH_TILES, \n",
    "                              sr_factor=SR_FACTOR, \n",
    "                              by_partition=True,\n",
    "                              ms_tile_size_train_val_test=(MS_SIZE['train'], MS_SIZE['val'], MS_SIZE['test']), \n",
    "                              cloud_sea_removal=CLOUD_SEA_REMOVAL, \n",
    "                              cloud_sea_weights_path=CLOUD_SEA_WEIGHTS_PATH, \n",
    "                              cloud_sea_pred_cutoff=CLOUD_SEA_PRED_CUTOFF,\n",
    "                              cloud_sea_keep_rate=CLOUD_SEA_KEEP_RATE,\n",
    "                              save_meta_to_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TILE_DENSITY_MAPS:\n",
    "    for row in meta.iterrows():\n",
    "        img_uid = row[0]\n",
    "        density = tile_density_map(DATA_PATH_TILES, \n",
    "                                   row[1], \n",
    "                                   pan_or_ms='pan',\n",
    "                                   density_dtype='uint8',\n",
    "                                   write_to_disk=True,\n",
    "                                   write_dir=DATA_PATH_TILES + '/density-maps', \n",
    "                                   write_filename=img_uid)\n",
    "    # Plot last density\n",
    "    plt.imshow(density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if CALCULATE_STATS:\n",
    "    train_tiles_mean, train_tiles_sd = mean_sd_of_train_tiles(DATA_PATH_TILES, \n",
    "                                                              sample_proportion=1.0, \n",
    "                                                              write_json=True)\n",
    "else:\n",
    "    train_tiles_mean, train_tiles_sd = read_mean_sd_json(DATA_PATH_TILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data input pipeline from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = {'train': n_tiles['train'],  # 100\n",
    "                       'val': n_tiles['val'],  # 100\n",
    "                       'test': n_tiles['test']}  # 100\n",
    "\n",
    "train_val_test = 'train'\n",
    "sensor = SENSORS[train_val_test]\n",
    "ds_train = {sensor: GeotiffDataset(tiles_path=DATA_PATH_TILES_P[train_val_test], \n",
    "                                   batch_size=BATCH_SIZE[train_val_test], \n",
    "                                   ms_tile_shape=(MS_SIZE[train_val_test], MS_SIZE[train_val_test], N_MS_BANDS), \n",
    "                                   pan_tile_shape=(PAN_SIZE[train_val_test], PAN_SIZE[train_val_test], N_PAN_BANDS),\n",
    "                                   sensor=sensor,\n",
    "                                   band_selection=MS_BANDS_WV02_IDXS, \n",
    "                                   mean_correction=train_tiles_mean,\n",
    "                                   cache_memory=True,\n",
    "                                   cache_file=str(DATA_PATH_TILES + '/ds_' + EXPERIMENT + '-'\n",
    "                                                    + train_val_test + '-' + sensor + '_cache'), \n",
    "                                   repeat=True, \n",
    "                                   shuffle=True, \n",
    "                                   shuffle_buffer_size=SHUFFLE_BUFFER_SIZE[train_val_test])\n",
    "           }\n",
    "# Getting the scaled output range from the scaler. Needed to calculate PSNR and SSIM:\n",
    "scaled_range = ds_train[sensor].get_scaler_output_range(print_ranges=True)\n",
    "\n",
    "# Returning the actual tf.data.dataset object:\n",
    "ds_train[sensor] = ds_train[sensor].get_dataset()\n",
    "print(ds_train.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set can have several sensors and is organized in a dictionary\n",
    "# structure: ds_val = {sensor: dataset} ... ex: ds_val = {'WV02': dataset_with_only_WV02_images}\n",
    "train_val_test = 'val'\n",
    "ds_val = {}\n",
    "for sensor in SENSORS[train_val_test]:\n",
    "    if sensor == 'WV02':\n",
    "        band_indices = MS_BANDS_WV02_IDXS\n",
    "    elif sensor == 'GE01':\n",
    "        band_indices = MS_BANDS_GE01_IDXS\n",
    "    ds_val[sensor] = GeotiffDataset(tiles_path=DATA_PATH_TILES_P[train_val_test], \n",
    "                                    batch_size=BATCH_SIZE[train_val_test], \n",
    "                                    ms_tile_shape=(MS_SIZE[train_val_test], MS_SIZE[train_val_test], N_MS_BANDS), \n",
    "                                    pan_tile_shape=(PAN_SIZE[train_val_test], PAN_SIZE[train_val_test], N_PAN_BANDS),\n",
    "                                    sensor=sensor,\n",
    "                                    band_selection=band_indices, \n",
    "                                    mean_correction=train_tiles_mean,\n",
    "                                    cache_memory=True,\n",
    "                                    cache_file=str(DATA_PATH_TILES + '/ds_' + EXPERIMENT + '-'\n",
    "                                                   + train_val_test + '-' + sensor + '_cache'), \n",
    "                                    repeat=True, \n",
    "                                    shuffle=True, \n",
    "                                    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE[train_val_test])\n",
    "    ds_val[sensor] = ds_val[sensor].get_dataset()\n",
    "print(ds_val.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build preliminary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Bicubic baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicubic = build_deterministic_sr_model(upsample_factor=SR_FACTOR,\n",
    "                                       resize_method='bicubic',\n",
    "                                       loss='mean_absolute_error',\n",
    "                                       metrics=('PSNR', 'SSIM'),\n",
    "                                       scaled_range=scaled_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ESRGAN Generator model (pretrain version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRE_BUILD:\n",
    "    pretrain_model =  build_generator(pretrain_or_gan='pretrain', \n",
    "                                      pretrain_learning_rate=PRETRAIN_LR, \n",
    "                                      pretrain_loss_l1_l2=PRETRAIN_LOSS,\n",
    "                                      pretrain_beta_1=PRETRAIN_BETA_1, \n",
    "                                      pretrain_beta_2=PRETRAIN_BETA_2, \n",
    "                                      pretrain_metrics=('PSNR', 'SSIM'),\n",
    "                                      scaled_range=scaled_range, \n",
    "                                      n_channels_in=N_MS_BANDS, \n",
    "                                      n_channels_out=N_PAN_BANDS, \n",
    "                                      height_width_in=None,  # None will make network image size agnostic\n",
    "                                      n_filters=N_FILTERS, \n",
    "                                      n_blocks=N_BLOCKS)\n",
    "    # pretrain_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pretraining with L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PRETRAIN:\n",
    "    history = pretrain_esrgan(generator=pretrain_model,\n",
    "                              ds_train_dict=ds_train,\n",
    "                              epochs=PRE_EPOCHS,\n",
    "                              steps_per_epoch=PRE_TRAIN_STEPS,\n",
    "                              initial_epoch=0,\n",
    "                              validate=True,\n",
    "                              ds_val_dict=ds_val,\n",
    "                              val_steps=PRE_VAL_STEPS,\n",
    "                              model_name=EXPERIMENT + '-pre',\n",
    "                              tag=EXPERIMENT,\n",
    "                              log_tensorboard=True,\n",
    "                              tensorboard_logs_dir=LOGS_EXP_DIR + '/tb',\n",
    "                              save_models=True,\n",
    "                              models_save_dir=LOGS_EXP_DIR + '/models',\n",
    "                              save_weights_only=True,\n",
    "                              log_train_images=True,\n",
    "                              n_train_image_batches=TRAIN_N_BATCHES_SAVE,\n",
    "                              log_val_images=True,\n",
    "                              n_val_image_batches=VAL_N_BATCHES_SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the full ESRGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GAN_BUILD:\n",
    "    gan_model = build_esrgan_model(PRETRAIN_WEIGHTS_PATH,\n",
    "                                   n_channels_in=N_MS_BANDS, \n",
    "                                   n_channels_out=N_PAN_BANDS, \n",
    "                                   n_filters=N_FILTERS, \n",
    "                                   n_blocks=N_BLOCKS, \n",
    "                                   pan_shape=(PAN_SIZE['train'], PAN_SIZE['train'], N_PAN_BANDS),\n",
    "                                   G_lr=GAN_G_LR, \n",
    "                                   D_lr=GAN_D_LR, \n",
    "                                   G_beta_1=G_BETA_1, \n",
    "                                   G_beta_2=G_BETA_2, \n",
    "                                   D_beta_1=D_BETA_1, \n",
    "                                   D_beta_2=D_BETA_2,\n",
    "                                   G_loss_pixel_w=G_LOSS_PIXEL_W, \n",
    "                                   G_loss_pixel_l1_l2='l1',\n",
    "                                   G_loss_percep_w=G_LOSS_PERCEP_W, \n",
    "                                   G_loss_percep_l1_l2='l1', \n",
    "                                   G_loss_percep_layer=54,\n",
    "                                   G_loss_percep_before_act=True,\n",
    "                                   G_loss_generator_w=G_LOSS_GENERATOR_W,\n",
    "                                   metric_reg=False, \n",
    "                                   metric_ma=METRIC_MA, \n",
    "                                   metric_niqe=METRIC_NIQE, \n",
    "                                   ma_niqe_proportion=MA_NIQE_PROPORTION,\n",
    "                                   matlab_wd_path='modules/matlab',\n",
    "                                   scale_mean=train_tiles_mean, \n",
    "                                   scaled_range=scaled_range, \n",
    "                                   shave_width=SHAVE_WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if GAN_TRAIN:\n",
    "    history = gan_train_esrgan(esrgan_model=gan_model,\n",
    "                               ds_train_dict=ds_train,\n",
    "                               epochs=GAN_EPOCHS,\n",
    "                               steps_per_epoch=GAN_TRAIN_STEPS,\n",
    "                               initial_epoch=0,\n",
    "                               validate=True,\n",
    "                               ds_val_dict=ds_val,\n",
    "                               val_steps=GAN_VAL_STEPS,\n",
    "                               model_name=EXPERIMENT + '-gan',\n",
    "                               tag=EXPERIMENT,\n",
    "                               log_tensorboard=True,\n",
    "                               tensorboard_logs_dir=LOGS_EXP_DIR + '/tb',\n",
    "                               save_models=True,\n",
    "                               models_save_dir=LOGS_EXP_DIR + '/models',\n",
    "                               save_weights_only=True,\n",
    "                               log_train_images=True,\n",
    "                               n_train_image_batches=TRAIN_N_BATCHES_SAVE,\n",
    "                               log_val_images=True,\n",
    "                               n_val_image_batches=VAL_N_BATCHES_SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Data input pipelines for final evaluation\n",
    "\n",
    "The pipeline is modified to include the file paths of the tiles/patches so that it is possible to log performance metrics for individual files and by extension for individual satellite images.\n",
    "\n",
    "#### 8.1.1 Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set can have several sensors and is organized in a dictionary\n",
    "# structure: ds_val = {sensor: dataset} ... ex: ds_val = {'WV02': dataset_with_only_WV02_images}\n",
    "train_val_test = 'val'\n",
    "ds_val = {}\n",
    "for sensor in SENSORS[train_val_test]:\n",
    "    if sensor == 'WV02':\n",
    "        band_indices = MS_BANDS_WV02_IDXS\n",
    "    elif sensor == 'GE01':\n",
    "        band_indices = MS_BANDS_GE01_IDXS\n",
    "    ds_val[sensor] = GeotiffDataset(tiles_path=DATA_PATH_TILES_P[train_val_test], \n",
    "                                    batch_size=BATCH_SIZE[train_val_test], \n",
    "                                    ms_tile_shape=(MS_SIZE[train_val_test], MS_SIZE[train_val_test], N_MS_BANDS), \n",
    "                                    pan_tile_shape=(PAN_SIZE[train_val_test], PAN_SIZE[train_val_test], N_PAN_BANDS),\n",
    "                                    sensor=sensor,\n",
    "                                    band_selection=band_indices, \n",
    "                                    mean_correction=train_tiles_mean,\n",
    "                                    cache_memory=False,\n",
    "                                    cache_file=str(DATA_PATH_TILES + '/ds_' + EXPERIMENT + '-'\n",
    "                                                   + train_val_test + '-' + sensor + '_filepath_cache'), \n",
    "                                    repeat=False, \n",
    "                                    shuffle=False, \n",
    "                                    shuffle_buffer_size=0, #SHUFFLE_BUFFER_SIZE[train_val_test], \n",
    "                                    include_file_paths=True)\n",
    "    ds_val[sensor] = ds_val[sensor].get_dataset()\n",
    "print(ds_val.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test = 'test'\n",
    "ds_test = {}\n",
    "for sensor in SENSORS[train_val_test]:\n",
    "    if sensor == 'WV02':\n",
    "        band_indices = MS_BANDS_WV02_IDXS\n",
    "    elif sensor == 'GE01':\n",
    "        band_indices = MS_BANDS_GE01_IDXS\n",
    "    ds_test[sensor] = GeotiffDataset(tiles_path=DATA_PATH_TILES_P[train_val_test], \n",
    "                                     batch_size=BATCH_SIZE[train_val_test], \n",
    "                                     ms_tile_shape=(MS_SIZE[train_val_test], MS_SIZE[train_val_test], N_MS_BANDS), \n",
    "                                     pan_tile_shape=(PAN_SIZE[train_val_test], PAN_SIZE[train_val_test], N_PAN_BANDS),\n",
    "                                     sensor=sensor,\n",
    "                                     band_selection=band_indices, \n",
    "                                     mean_correction=train_tiles_mean,\n",
    "                                     cache_memory=False,\n",
    "                                     cache_file=str(DATA_PATH_TILES + '/ds_' + EXPERIMENT + '-'\n",
    "                                                    + train_val_test + '-' + sensor + '_filepath_cache'), \n",
    "                                     repeat=False, \n",
    "                                     shuffle=False, \n",
    "                                     shuffle_buffer_size=0)\n",
    "    ds_test[sensor] = ds_test[sensor].get_dataset()\n",
    "print(ds_test.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Evaluate last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_or_test = 'val'\n",
    "\n",
    "# Computing Ma is 100x more time consuming than anything else. It is not interesting to measure this for pretraining\n",
    "if METRIC_MA:\n",
    "    PRE_GAN = ['gan']\n",
    "else:\n",
    "    PRE_GAN = ['pre', 'gan']\n",
    "    \n",
    "for pre_gan in PRE_GAN:\n",
    "    for sensor in SENSORS[val_or_test]:\n",
    "        if sensor == 'GE01':\n",
    "            band_indices = MS_BANDS_GE01_IDXS\n",
    "        elif sensor == 'WV02':\n",
    "            band_indices = MS_BANDS_WV02_IDXS\n",
    "        if pre_gan == 'pre':\n",
    "            gan_model.G.load_weights(PRETRAIN_WEIGHTS_PATH)\n",
    "        else:\n",
    "            gan_model.G.load_weights(GAN_WEIGHTS_PATH)\n",
    "\n",
    "        start = time.time()\n",
    "        results_df = esrgan_evaluate(model=gan_model, \n",
    "                                     dataset=ds_val[sensor], \n",
    "                                     steps='all', \n",
    "                                     per_image=True, \n",
    "                                     write_csv=True,\n",
    "                                     csv_path=str(LOGS_EXP_DIR + '/csv/' + 'final_epoch-' \n",
    "                                                  + pre_gan + '-' + val_or_test + '-' + sensor + '.csv'), \n",
    "                                     verbose=1\n",
    "                                    )\n",
    "        end = time.time()\n",
    "        print(str((end - start) / 60), 'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Evaluate every kth epoch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if PRE_EVALUATE or GAN_EVALUATE:\n",
    "    esrgan_epoch_evaluator(gan_model,\n",
    "                           model_weights_dir=PRETRAIN_WEIGHTS_DIR,\n",
    "                           model_weight_prefix=EVAL_PREFIX,\n",
    "                           dataset=ds_val[EVAL_SENSOR],\n",
    "                           n_epochs=EVAL_N_EPOCHS,\n",
    "                           first_epoch=EVAL_FIRST_STEP,\n",
    "                           steps_per_epoch=EVAL_STEPS_PER_EPOCH,\n",
    "                           csv_dir=LOGS_EXP_DIR + '/csv/' + EVAL_SENSOR, \n",
    "                           per_image=EVAL_PER_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Comparison plots"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MS_IMG_PATH = 'data/toulon-laspezia-tiles/e01/val/GE01_La_Spezia_2017_04_16_011651188010_0/ms/00031.tif'\n",
    "\n",
    "RESULTS_DIR = 'results/test'\n",
    "SENSOR = 'GE01'\n",
    "\n",
    "for pre_gan in ['pre', 'gan']:\n",
    "    if SENSOR == 'GE01':\n",
    "        band_indices = MS_BANDS_GE01_IDXS\n",
    "    elif SENSOR == 'WV02':\n",
    "        band_indices = MS_BANDS_WV02_IDXS\n",
    "    if pre_gan == 'pre':\n",
    "        pretrain_model.load_weights(PRETRAIN_WEIGHTS_PATH)\n",
    "    else:\n",
    "        pretrain_model.load_weights(GAN_WEIGHTS_PATH)\n",
    "    sr_img = esrgan_predict(model=pretrain_model, \n",
    "                            ms_img_path=MS_IMG_PATH, \n",
    "                            result_dir=RESULTS_DIR, \n",
    "                            sensor=SENSOR, \n",
    "                            band_indices=band_indices, \n",
    "                            mean_correction=train_tiles_mean, \n",
    "                            pre_or_gan=pre_gan,\n",
    "                            sr_factor=SR_FACTOR, \n",
    "                            copy_pan_img=True,\n",
    "                            pan_img_path=None, \n",
    "                            output_dtype='uint16', \n",
    "                            geotiff_or_png='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
