{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRCNN on MNIST and CIFAR-10\n",
    "\n",
    "This is an implementation of SRCNN that mostly follows the core setup presented in the [original paper](https://arxiv.org/pdf/1501.00092v3.pdf). Main differences:\n",
    "\n",
    "- Different optimizer, Adam, as opposed to SGD.\n",
    "- The original paper only presents results on color images and primarily uses a preprocessing *trick* transforming from RGB color space to YCbCr color space and only training on Y (illuminance). They also in the end present good results working directly on the RGB channels. The latter is what is done in this implementation.\n",
    "- In the paper they trained on sub-images of images in the ImageNet dataset.\n",
    "- The paper is not using *same* zero padding to keep the dimensions of the output image equal to the dimensions of the input. Their way of ensuring this is a bit involved, and to my eyes zero padding seems to be working (at least on MNIST).\n",
    "\n",
    "Results on CIFAR-10 is not the most impressive, but I believe this is mainly due to the very low resolution of these images compared to what they actually are depicting. This is probably a dataset pretty unsuitable for SR.\n",
    "\n",
    "https://paperswithcode.com/paper/image-super-resolution-using-deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "# Check GPUs:\",\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Prevent TensorFlow from allocating all memory of all GPUs:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the dataset you want to run on\n",
    "DATASET = 'MNIST'\n",
    "#DATASET = 'CIFAR-10'\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "DOWNSAMPLE_FACTOR = 4\n",
    "DOWNSAMPLE_METHOD = cv2.INTER_CUBIC\n",
    "#HEIGHT, WIDTH = (28, 28)\n",
    "HEIGHT, WIDTH = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset (MNIST or CIFAR-10)\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'MNIST':\n",
    "    (hr_train, train_labels) , (hr_test, test_labels) = mnist.load_data()\n",
    "\n",
    "elif DATASET == 'CIFAR-10':\n",
    "    (hr_train, train_labels) , (hr_test, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample (and upsample) images\n",
    "\n",
    "Currently downsampling and upsampling to degrade the image (this is most in line with the original paper).\n",
    "\n",
    "- x is input to the neural net and is the degraded (downsampled+upsampled) image\n",
    "- y is ground truth (the HR, undegraded image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_image(img, factor):\n",
    "    #print(img.shape)\n",
    "    if len(img.shape) == 3:\n",
    "        h, w, _ = img.shape\n",
    "    elif len(img.shape) == 2:\n",
    "        h, w = img.shape\n",
    "        \n",
    "    new_height = int(h / factor)\n",
    "    new_width = int(w / factor)\n",
    "\n",
    "    # resize the image - down\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation = DOWNSAMPLE_METHOD)\n",
    "\n",
    "    # resize the image - up\n",
    "    img = cv2.resize(img, (w, h), interpolation = DOWNSAMPLE_METHOD)\n",
    "\n",
    "    # save the image\n",
    "    #print(img.shape)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_images_tf(imgs, factor):\n",
    "    imgs = tf.convert_to_tensor(imgs)\n",
    "    #print(img.shape)\n",
    "    if len(imgs.shape) == 4:\n",
    "        _, h, w, _ = imgs.shape\n",
    "    elif len(imgs.shape) == 3:\n",
    "        _, h, w = imgs.shape\n",
    "        \n",
    "    new_height = int(h / factor)\n",
    "    new_width = int(w / factor)\n",
    "    \n",
    "    # resize the image - down\n",
    "    tf.image.resize(imgs, (new_height, new_width), \n",
    "                    method=tf.image.ResizeMethod.BICUBIC, preserve_aspect_ratio=False,\n",
    "                    antialias=False, name=None)\n",
    "    \n",
    "    # resize the image - up\n",
    "    tf.image.resize(imgs, (h, w), \n",
    "                    method=tf.image.ResizeMethod.BICUBIC, preserve_aspect_ratio=False,\n",
    "                    antialias=False, name=None)\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_tf = degrade_images_tf(lr_train, DOWNSAMPLE_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train = np.array([degrade_image(image, DOWNSAMPLE_FACTOR) for image in hr_train])\n",
    "lr_test = np.array([degrade_image(image, DOWNSAMPLE_FACTOR) for image in hr_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really just scaling, not normalizing\n",
    "# From 0-255 ints to 0-1 floats\n",
    "def normalize(ndarray):\n",
    "    return ndarray.astype('float32')/255\n",
    "\n",
    "# From 0-1 floats to 0-255 ints\n",
    "def denormalize(ndarray):\n",
    "    input_min = np.amin(ndarray)\n",
    "    input_max = np.amax(ndarray)\n",
    "    input_range = input_max - input_min\n",
    "    scaled = np.array((ndarray - input_min) / float(input_range), dtype=np.float32)\n",
    "    return np.array(scaled * 255, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train = normalize(lr_train)\n",
    "lr_test = normalize(lr_test)\n",
    "hr_train = normalize(hr_train)\n",
    "hr_test = normalize(hr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Expanding dimensions)\n",
    "\n",
    "Necessary to be compatible with vanilla [Conv2D tf.keras layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'MNIST':\n",
    "    lr_train = np.expand_dims(lr_train, -1)\n",
    "    lr_test = np.expand_dims(lr_test, -1)\n",
    "    hr_train = np.expand_dims(hr_train, -1)\n",
    "    hr_test = np.expand_dims(hr_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "It is really quite straight-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_srcnn(channels_in, channels_out):\n",
    "    \n",
    "    srcnn = Sequential()\n",
    "    \n",
    "    srcnn.add(Conv2D(filters=128, kernel_size = (9, 9), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None),\n",
    "                     bias_initializer='zeros',\n",
    "                     activation='relu', padding='same', use_bias=True, \n",
    "                     input_shape=(HEIGHT, WIDTH, channels_in)))\n",
    "\n",
    "    srcnn.add(Conv2D(filters=64, kernel_size = (1, 1), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None),\n",
    "                     bias_initializer='zeros',\n",
    "                     activation='relu', padding='same', use_bias=True))\n",
    "    \n",
    "    srcnn.add(Conv2D(filters=channels_out, kernel_size = (5, 5), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None), \n",
    "                     bias_initializer='zeros',\n",
    "                     activation='linear', padding='same', use_bias=True))\n",
    "    \n",
    "    # define optimizer\n",
    "    adam = Adam(lr=0.0003)\n",
    "    \n",
    "    # compile model\n",
    "    srcnn.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return srcnn\n",
    "\n",
    "if DATASET == 'MNIST':\n",
    "    srcnn = build_srcnn(channels_in = 1, channels_out = 1)\n",
    "elif DATASET == 'CIFAR-10':\n",
    "    srcnn = build_srcnn(channels_in = 3, channels_out = 3)\n",
    "srcnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = srcnn.fit(lr_train, \n",
    "                    hr_train, epochs = EPOCHS, \n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    validation_data = (lr_test, hr_test)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anecdotal comparison of predict vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(lr, hr, label):\n",
    "    sr = srcnn.predict(np.expand_dims(lr, axis = 0))[0,:,:,:]\n",
    "    sr = denormalize(sr)\n",
    "    lr = denormalize(lr)\n",
    "    hr = denormalize(hr)\n",
    "    if lr.shape[2] == 1:\n",
    "        lr = lr[:,:,0]\n",
    "        sr = sr[:,:,0]\n",
    "        hr = hr[:,:,0]\n",
    "        cmap = 'gray'\n",
    "    elif lr.shape[2] == 3:\n",
    "        cmap = None\n",
    "\n",
    "    fig = plt.figure(figsize = (10,10))\n",
    "    \n",
    "    ax0 = fig.add_subplot(1,3,1)\n",
    "    ax0.set_title('LR' + ', bicubic interpolation')\n",
    "    ax0 = plt.imshow(lr, cmap = cmap)\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,3,2)\n",
    "    ax1.set_title('SRCNN')\n",
    "    ax1 = plt.imshow(sr, cmap = cmap)\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,3,3)\n",
    "    ax2.set_title('HR (ground truth) - ' + str(label))\n",
    "    ax2 = plt.imshow(hr, cmap = cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idxs = np.random.randint(0, len(lr_test), size = 20)\n",
    "for idx in random_idxs:\n",
    "    plot_comparison(lr_test[idx,:,:,:], hr_test[idx,:,:,:], test_labels[idx])"
   ]
  }
 ],
 "metadata": {
  "keep_output": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
