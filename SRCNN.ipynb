{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRCNN - 2014\n",
    "\n",
    "https://paperswithcode.com/paper/image-super-resolution-using-deep\n",
    "\n",
    "This is a relatively simple model. Results on the satellite imagery is currently not very impressive and it is not certain that it beats bicubic convolution when compared with PSNR and SSIM as metrics. However, I have done minimal tuning and have not gotten rid of the border effect that SRCNN introduces out of the box.\n",
    "\n",
    "Primary use of the model to date has been to validate the data generation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import rasterio\n",
    "import rasterio.plot\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Check GPUs:\",\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Prevent TensorFlow from allocating all memory of all GPUs:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to location where individual satellite images are located\n",
    "DATA_PATH = 'data/toulon-laspezia' \n",
    "\n",
    "# Paths to the tiled imagery\n",
    "DATA_PATH_TILES = 'data/toulon-laspezia-tiles'\n",
    "DATA_PATH_TILES_TRAIN = str(DATA_PATH_TILES + '/train')\n",
    "DATA_PATH_TILES_VAL = str(DATA_PATH_TILES + '/val')\n",
    "DATA_PATH_TILES_TEST = str(DATA_PATH_TILES + '/test')\n",
    "\n",
    "# Loading the metadata geopandas df produced when generating tiles to disk\n",
    "with open(str(DATA_PATH_TILES + '/metadata_tile_allocation.pickle'), 'rb') as file:\n",
    "    meta = pickle.load(file)\n",
    "    \n",
    "N_IMAGES = len(meta.index)\n",
    "N_IMAGES_TRAIN = meta['train_val_test'].value_counts()['train']\n",
    "N_IMAGES_VAL = meta['train_val_test'].value_counts()['val']\n",
    "N_IMAGES_TEST = meta['train_val_test'].value_counts()['test']\n",
    "print('Number of satellite images - train:', N_IMAGES_TRAIN, \n",
    "      ', val:', N_IMAGES_VAL, ', test:', N_IMAGES_TEST)\n",
    "\n",
    "N_TILES_TRAIN = meta.loc[meta['train_val_test'] == 'train', 'n_tiles'].sum()\n",
    "N_TILES_VAL = meta.loc[meta['train_val_test'] == 'val', 'n_tiles'].sum()\n",
    "N_TILES_TEST = meta.loc[meta['train_val_test'] == 'test', 'n_tiles'].sum()\n",
    "print('Number of satellite image tiles - train:', N_TILES_TRAIN, \n",
    "      ', val:', N_TILES_VAL, ', test:', N_TILES_TEST)\n",
    "\n",
    "PAN_WIDTH, PAN_HEIGHT = (384, 384)\n",
    "\n",
    "SR_FACTOR = 4\n",
    "MS_WIDTH, MS_HEIGHT = (int(PAN_WIDTH/SR_FACTOR), int(PAN_HEIGHT/SR_FACTOR))\n",
    "\n",
    "# Should be derived automatically, but added here as a quick fix\n",
    "MS_BANDS = 8\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tile generator from disk\n",
    "\n",
    "Using `tf.data` API to construct a `Dataset` generator reading and preprocessing tiles from disk.\n",
    "\n",
    "Best practices from https://www.tensorflow.org/guide/data, including multithreading, prefetching, shuffling, batching and caching.\n",
    "\n",
    "`rasterio` is used to read geotiffs. the `decode_geotiff()` function is run inside a `tf.py_function()` wrapper ensuring that this function is also run in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_geotiff(image_path):\n",
    "    image_path = pathlib.Path(image_path.numpy().decode())\n",
    "    with rasterio.open(image_path) as src:\n",
    "        img = src.read()\n",
    "    img = rasterio.plot.reshape_as_image(img) # from channels first to channels last\n",
    "    return img\n",
    "\n",
    "def preprocess_images(img, ms_or_pan):\n",
    "    if ms_or_pan == 'ms':\n",
    "        h, w = MS_HEIGHT, MS_WIDTH\n",
    "    elif ms_or_pan == 'pan':\n",
    "        h, w = PAN_HEIGHT, PAN_WIDTH\n",
    "        \n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.reshape(img, [h, w, -1]) # To avoid issue with extra dimension\n",
    "    return img\n",
    "\n",
    "def upsample_images(ms_img, pan_img):\n",
    "    ms_img = tf.ensure_shape(ms_img, [MS_HEIGHT, MS_WIDTH, MS_BANDS])\n",
    "    ms_img = tf.image.resize(ms_img, [PAN_HEIGHT, PAN_WIDTH])\n",
    "    return ms_img, pan_img\n",
    "\n",
    "def process_path(ms_tile_path):\n",
    "    img_string_UID = tf.strings.split(ms_tile_path, os.sep)[-3]\n",
    "    tile_UID = tf.strings.split(tf.strings.split(ms_tile_path, os.sep)[-1], '.')[0]\n",
    "    \n",
    "    ms_img = tf.py_function(decode_geotiff, [ms_tile_path], [tf.int16])\n",
    "    pan_tile_path = tf.strings.regex_replace(ms_tile_path, '\\\\\\\\ms\\\\\\\\', '\\\\\\\\pan\\\\\\\\')\n",
    "    pan_img = tf.py_function(decode_geotiff, [pan_tile_path], [tf.int16])\n",
    "    \n",
    "    ms_img = preprocess_images(ms_img, 'ms')\n",
    "    pan_img = preprocess_images(pan_img, 'pan')\n",
    "    \n",
    "    return ms_img, pan_img\n",
    "\n",
    "# https://www.tensorflow.org/tutorials/load_data/images\n",
    "def prepare_for_training(ds, batch_size, cache=True, shuffle_buffer_size=100):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_tif_tiles(tiles_path, batch_size, upsampling = False, \n",
    "                           cache = True, shuffle_buffer_size = 1000):\n",
    "    \n",
    "    ds = tf.data.Dataset.list_files(str(pathlib.Path(tiles_path)/'*/ms*.tif'))\n",
    "    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # upsampling through bicubic convolution before SR is required for SRCNN\n",
    "    if upsampling:\n",
    "        ds = ds.map(upsample_images, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds = prepare_for_training(ds, batch_size, cache, shuffle_buffer_size)\n",
    "    return ds\n",
    "\n",
    "# SRCNN needs upsampling!\n",
    "ds_train = dataset_from_tif_tiles(DATA_PATH_TILES_TRAIN, BATCH_SIZE, upsampling = True)\n",
    "ds_val = dataset_from_tif_tiles(DATA_PATH_TILES_VAL, BATCH_SIZE, upsampling = True)\n",
    "ds_test = dataset_from_tif_tiles(DATA_PATH_TILES_TEST, BATCH_SIZE, upsampling = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch):\n",
    "    ms = image_batch[0].numpy()\n",
    "    pan = image_batch[1].numpy()\n",
    "    print('ms batch shape', ms.shape)\n",
    "    print('pan batch shape', pan.shape)\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for i in range(8):\n",
    "        i = i * 2\n",
    "        ax_ms = plt.subplot(4,4,i+1, label = 'ms')\n",
    "        \n",
    "        ms_image = ms[i,:,:,2] # Just showing channel 2 as grayscale\n",
    "        pan_image = pan[i,:,:,0]\n",
    "\n",
    "        #plt.imshow(ms_image)\n",
    "        ax_ms.imshow(ms_image, cmap = 'gray')\n",
    "        \n",
    "        ax_pan = plt.subplot(4,4,i+2, label = 'pan')\n",
    "\n",
    "        ax_pan.imshow(pan_image, cmap = 'gray')\n",
    "        \n",
    "show_batch(next(iter(ds_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_srcnn(channels_in, channels_out):\n",
    "    \n",
    "    srcnn = Sequential()\n",
    "    \n",
    "    srcnn.add(Conv2D(filters=128, kernel_size = (9, 9), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None),\n",
    "                     bias_initializer='zeros',\n",
    "                     activation='relu', padding='same', use_bias=True, \n",
    "                     input_shape=(None, None, channels_in)))\n",
    "\n",
    "    srcnn.add(Conv2D(filters=64, kernel_size = (1, 1), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None),\n",
    "                     bias_initializer='zeros',\n",
    "                     activation='relu', padding='same', use_bias=True))\n",
    "    \n",
    "    srcnn.add(Conv2D(filters=channels_out, kernel_size = (5, 5), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None), \n",
    "                     bias_initializer='zeros',\n",
    "                     activation='linear', padding='same', use_bias=True))\n",
    "    \n",
    "    # define optimizer\n",
    "    adam = Adam(lr=0.0003)\n",
    "    \n",
    "    # compile model\n",
    "    srcnn.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return srcnn\n",
    "\n",
    "srcnn = build_srcnn(channels_in = MS_BANDS, channels_out = 1)\n",
    "srcnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = srcnn.fit(ds_train,\n",
    "                    epochs = EPOCHS,\n",
    "                    validation_data = ds_val,\n",
    "                    steps_per_epoch = 100, \n",
    "                    validation_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#srcnn.load_weights('models/model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(img1, img2):\n",
    "    img1 = np.expand_dims(img1, -1)\n",
    "    img2 = np.expand_dims(img2, -1)\n",
    "    return tf.image.psnr(img1, img2, max_val=1.0)\n",
    "\n",
    "def ssim(img1, img2):\n",
    "    img1 = tf.convert_to_tensor(np.expand_dims(img1, -1), dtype = tf.float32)\n",
    "    img2 = tf.convert_to_tensor(np.expand_dims(img2, -1), dtype = tf.float32)\n",
    "    return tf.image.ssim(img1, img2, max_val=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparisons(model, ds, n_comparisons):\n",
    "    batch = next(iter(ds))\n",
    "    \n",
    "    ms = batch[0]\n",
    "    ms = tf.math.reduce_mean(ms, axis = -1).numpy()\n",
    "        \n",
    "    pan = batch[1].numpy()[:,:,:,0]\n",
    "    \n",
    "    sr = model.predict(batch)[:,:,:,0]\n",
    "    \n",
    "    print(ms.shape)\n",
    "    print(pan.shape)\n",
    "    print(sr.shape)\n",
    "    \n",
    "    cmap = 'gray'\n",
    "\n",
    "    for i in range(n_comparisons):        \n",
    "        fig, axs = plt.subplots(2, 2, constrained_layout=True, figsize = (20,20))\n",
    "        fig.suptitle(str('Comparison ' + str(i)))\n",
    "        \n",
    "        axs[0,0].set_title('MS Bicubic Upsampling ' + \n",
    "                           'PSNR: ' + \n",
    "                           str(psnr(ms[i], pan[i]).numpy()) + \n",
    "                           ' / SSIM: ' +\n",
    "                           str(ssim(ms[i], pan[i]).numpy()))\n",
    "        axs[0,0].imshow(ms[i], cmap = cmap)\n",
    "\n",
    "        axs[0,1].set_title('SRCNN ' + \n",
    "                           'PSNR: ' + \n",
    "                           str(psnr(sr[i], pan[i]).numpy()) + \n",
    "                           ' / SSIM: ' +\n",
    "                           str(ssim(sr[i], pan[i]).numpy()))\n",
    "        axs[0,1].imshow(sr[i], cmap = cmap)\n",
    "\n",
    "        axs[1,0].set_title('PAN (Ground Truth)')\n",
    "        axs[1,0].imshow(pan[i], cmap = cmap)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comparisons = 16\n",
    "\n",
    "plot_comparisons(srcnn, ds_val, n_comparisons = n_comparisons)"
   ]
  }
 ],
 "metadata": {
  "keep_output": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
