{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drafty implementation of SRCNN\n",
    "\n",
    "https://arxiv.org/pdf/1501.00092v3.pdf\n",
    "\n",
    "https://paperswithcode.com/paper/image-super-resolution-using-deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "# Check GPUs:\",\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # Prevent TensorFlow from allocating all memory of all GPUs:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "DOWNSAMPLE_FACTOR = 4\n",
    "DOWNSAMPLE_METHOD = cv2.INTER_CUBIC\n",
    "#HEIGHT, WIDTH = (28, 28)\n",
    "HEIGHT, WIDTH = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train, train_labels) , (y_test, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample (and upsample) images\n",
    "\n",
    "Currently downsampling and upsampling to degrade the image (this is most in line with the original paper).\n",
    "\n",
    "- x is input to the neural net and is the degraded (downsampled+upsampled) image\n",
    "- y is ground truth (the HR, undegraded image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_image(img, factor):\n",
    "    #print(img.shape)\n",
    "    h, w = img.shape\n",
    "    new_height = int(h / factor)\n",
    "    new_width = int(w / factor)\n",
    "\n",
    "    # resize the image - down\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation = DOWNSAMPLE_METHOD)\n",
    "\n",
    "    # resize the image - up\n",
    "    img = cv2.resize(img, (w, h), interpolation = DOWNSAMPLE_METHOD)\n",
    "\n",
    "    # save the image\n",
    "    #print(img.shape)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([degrade_image(image, DOWNSAMPLE_FACTOR) for image in y_train])\n",
    "x_test = np.array([degrade_image(image, DOWNSAMPLE_FACTOR) for image in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Expanding dimensions)\n",
    "\n",
    "Necessary to be compatible with vanilla Conv2D Keras layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "y_test = np.expand_dims(y_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "It is really quite straight-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_srcnn():\n",
    "    \n",
    "    srcnn = Sequential()\n",
    "    \n",
    "    srcnn.add(Conv2D(filters=128, kernel_size = (9, 9), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None),\n",
    "                     bias_initializer='zeros',\n",
    "                     activation='relu', padding='same', use_bias=True, input_shape=(HEIGHT, WIDTH, 1)))\n",
    "    \n",
    "    srcnn.add(Conv2D(filters=64, kernel_size = (1, 1), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None),\n",
    "                     bias_initializer='zeros',\n",
    "                     activation='relu', padding='same', use_bias=True))\n",
    "    \n",
    "    srcnn.add(Conv2D(filters=1, kernel_size = (5, 5), \n",
    "                     kernel_initializer=RandomNormal(mean=0.0, stddev=0.001, seed=None), \n",
    "                     bias_initializer='zeros',\n",
    "                     activation='linear', padding='same', use_bias=True))\n",
    "    \n",
    "    # define optimizer\n",
    "    adam = Adam(lr=0.0003)\n",
    "    \n",
    "    # compile model\n",
    "    srcnn.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return srcnn\n",
    "\n",
    "srcnn = build_srcnn()\n",
    "srcnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = srcnn.fit(x_train, \n",
    "                    y_train, epochs = EPOCHS, \n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    validation_data = (x_test, y_test)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anecdotal comparison of predict vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(x, y, label):\n",
    "    sr = srcnn.predict(np.expand_dims(x, axis = 0))\n",
    "    #print(sr.shape)\n",
    "    #print(x.shape, y.shape)\n",
    "    fig = plt.figure(figsize = (10,10))\n",
    "    \n",
    "    ax0 = fig.add_subplot(1,3,1)\n",
    "    ax0.set_title('LR' + ', bicubic interpolation')\n",
    "    ax0 = plt.imshow(x[:,:,0], cmap = 'gray')\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,3,2)\n",
    "    ax1.set_title('SRCNN')\n",
    "    ax1 = plt.imshow(sr[0,:,:,0], cmap = 'gray')\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,3,3)\n",
    "    ax2.set_title('HR (ground truth) - ' + str(label))\n",
    "    ax2 = plt.imshow(y[:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_test = len(x_test)\n",
    "random_idxs = np.random.randint(0, len_test, size = 20)\n",
    "for idx in random_idxs:\n",
    "    plot_comparison(x_test[idx,:,:,:], y_test[idx,:,:,:], test_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is only scratchpad - not to be run\n",
    "https://github.com/Shritesh99/Image_Super_Resolution/blob/master/SRCNN.ipynb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def psnr(target, ref):\n",
    "         \n",
    "    # assume RGB image\n",
    "    target_data = target.astype(float)\n",
    "    ref_data = ref.astype(float)\n",
    "    print(target_data.shape)\n",
    "    print(ref_data.shape)\n",
    "    diff = ref_data - target_data\n",
    "    \n",
    "    diff = diff.flatten('C')\n",
    "    \n",
    "    rmse = math.sqrt(np.mean(diff ** 2.))\n",
    "\n",
    "    return 20 * math.log10(255. / rmse)\n",
    "\n",
    "def mse(target, ref):\n",
    "    # the MSE between the two images is the sum of the squared difference between the two images\n",
    "    err = np.sum((target.astype('float') - ref.astype('float')) ** 2)\n",
    "    err /= float(target.shape[0] * target.shape[1])\n",
    "    \n",
    "    return err\n",
    "\n",
    "def compare_images(target, ref):\n",
    "    scores = []\n",
    "    scores.append(psnr(target, ref))\n",
    "    scores.append(mse(target, ref))\n",
    "    scores.append(ssim(target, ref, multichannel =True))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def modcrop(img, scale):\n",
    "    tmpsz = img.shape\n",
    "    sz = tmpsz[0:2]\n",
    "    print(sz, tmpsz)\n",
    "    sz = sz - np.mod(sz, scale)\n",
    "    print(sz, tmpsz)\n",
    "    img = img[0:sz[0], 1:sz[1]]\n",
    "    return img\n",
    "\n",
    "def shave(image, border):\n",
    "    img = image[border: -border, border: -border]\n",
    "    return img\n",
    "\n",
    "def preprocess(image_path):\n",
    "    # load the degraded and reference images\n",
    "    image_path = Path(image_path)\n",
    "    path = image_path.parent\n",
    "    print(path)\n",
    "    file = image_path.name\n",
    "    print(file)\n",
    "    degraded_image_path = path.joinpath('degraded', file)\n",
    "    print(degraded_image_path)\n",
    "    print(image_path)\n",
    "    degraded = cv2.imread(str(degraded_image_path))\n",
    "    ref = cv2.imread(str(image_path))\n",
    "    plt.imshow(degraded)\n",
    "    #print(degraded.shape)\n",
    "        \n",
    "    # preprocess the image with modcrop\n",
    "    ref = modcrop(ref, 3)\n",
    "    degraded = modcrop(degraded, 3)\n",
    "    #print(degraded.shape)\n",
    "    \n",
    "    # convert the image to YCrCb - (srcnn trained on Y channel)\n",
    "    temp = cv2.cvtColor(degraded, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    # create image slice and normalize  \n",
    "    Y = np.zeros((1, temp.shape[0], temp.shape[1], 1), dtype=float)\n",
    "    Y[0, :, :, 0] = temp[:, :, 0].astype(float) / 255\n",
    "    \n",
    "    return Y\n",
    "\n",
    "y = preprocess('data/Set5/woman.png')\n",
    "plt.imshow(y[0,:,:,0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def degrade_image_file(path, extension, factor):\n",
    "    path = Path(path)\n",
    "    path_degraded = path.joinpath('degraded')\n",
    "    path_degraded.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # loop through the files in the directory\n",
    "    for file in os.listdir(path):\n",
    "        # open the file\n",
    "        full_path = path.joinpath(file)\n",
    "        if full_path.suffix != extension: continue\n",
    "        img = cv2.imread(str(path.joinpath(file)))\n",
    "        \n",
    "        # find old and new image dimensions\n",
    "        h, w, _ = img.shape\n",
    "        new_height = int(h / factor)\n",
    "        new_width = int(w / factor)\n",
    "        \n",
    "        # resize the image - down\n",
    "        img = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_LINEAR)\n",
    "        \n",
    "        # resize the image - up\n",
    "        img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n",
    "        \n",
    "        # save the image\n",
    "        save_success = cv2.imwrite(str(path_degraded.joinpath(file)), img)\n",
    "        print('Saved image', file, '------ Successful (True/False)?', save_success)\n",
    "        #plt.imshow(img)\n",
    "        \n",
    "degrade_image_file('data/set5', '.png', 2)"
   ]
  }
 ],
 "metadata": {
  "keep_output": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
